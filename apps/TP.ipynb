{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "\n",
    "def distributed_init():\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    return local_rank, world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_free_port():\n",
    "    \"\"\" https://stackoverflow.com/questions/1365265/on-localhost-how-do-i-pick-a-free-port-number \"\"\"\n",
    "    import socket\n",
    "    from contextlib import closing\n",
    "\n",
    "    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
    "        s.bind(('', 0))\n",
    "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        return str(s.getsockname()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 2\n",
    "master_addr = '127.0.0.1'\n",
    "master_port = find_free_port()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'57727'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "rank = 0\n",
    "backend = 'nccl'\n",
    "\n",
    "print(f'setting up {rank=} {world_size=} {backend=}')\n",
    "\n",
    "# set up the master's ip address so this child process can coordinate\n",
    "os.environ['MASTER_ADDR'] = master_addr\n",
    "os.environ['MASTER_PORT'] = master_port\n",
    "print(f\"{master_addr=} {master_port=}\")\n",
    "\n",
    "# Initializes the default distributed process group, and this will also initialize the distributed package.\n",
    "dist.init_process_group(backend, rank=rank, world_size=world_size, timeout=timedelta(seconds=30), init_method=\"env://\")\n",
    "print(f\"{rank=} init complete\")\n",
    "dist.destroy_process_group()\n",
    "print(f\"{rank=} destroy complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6,7 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 384 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 512 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 640 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 768 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 896 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1024 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1152 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1280 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1408 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1536 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1664 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1792 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 1920 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2048 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2176 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2304 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2432 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2560 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2688 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2816 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 2944 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3072 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3200 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3328 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3456 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3584 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3712 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3840 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 3968 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4096 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4224 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4352 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4480 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4608 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4736 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4864 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 4992 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5120 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5248 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5376 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5504 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5632 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5760 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 5888 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6016 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6144 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6272 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6400 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6528 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6656 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6784 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 6912 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7040 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7168 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7296 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7424 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7552 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7680 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7808 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 7936 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8064 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8192 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8320 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8448 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8576 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8704 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8832 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 8960 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9088 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9216 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9344 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9472 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9600 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9728 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9856 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 9984 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10112 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10240 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10368 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10496 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10624 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10752 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 10880 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11008 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11136 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11264 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11392 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11520 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11648 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11776 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 11904 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 12032 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 12160 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 12288 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 12416 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 12544 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n",
      "CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill 122880 --budget 12672 --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 100):\n",
    "    print(f\"CUDA_VISIBLE_DEVICES=0 python test/e2e_ablation.py --prefill {122880} --budget {128*i} --chunk_size 8 --top_p 0.9 --temp 0.6 --gamma 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"JackFram/llama-68m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a much a my some a'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([263, 1568,  263,  590,  777,  263])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 128 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 64 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 32 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 16 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 8 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 4 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 2 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,8,9 OMP_NUM_THREADS=48 torchrun --nproc_per_node=8 test/TP_baseline.py --prefill 128 --bsz 1 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 128 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 64 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 32 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 16 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 8 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 4 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 2 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2,3,4 OMP_NUM_THREADS=48 torchrun --nproc_per_node=4 test/TP_baseline.py --prefill 128 --bsz 1 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 128 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 64 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 32 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 16 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 8 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 4 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 2 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1,2 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 1 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 128 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 64 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 32 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 16 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 8 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 4 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 2 --gen_len 32\n",
      "CUDA_VISIBLE_DEVICES=1 OMP_NUM_THREADS=48 torchrun --nproc_per_node=1 test/TP_baseline.py --prefill 128 --bsz 1 --gen_len 32\n"
     ]
    }
   ],
   "source": [
    "bsz_list = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "cuda_list = ['1', '1,2', '1,2,3,4', '1,2,3,4,5,6,8,9']\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=5,6 OMP_NUM_THREADS=48 torchrun --nproc_per_node=2 test/TP_baseline.py --prefill 128 --bsz 1 --gen_len 32\n",
    "\n",
    "for cuda in reversed(cuda_list):\n",
    "    for bsz in reversed(bsz_list):\n",
    "        print(f\"CUDA_VISIBLE_DEVICES={cuda} OMP_NUM_THREADS=48 torchrun --nproc_per_node={len(cuda)//2+1} test/TP_baseline.py --prefill 128 --bsz {bsz} --gen_len 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gt = torch.load('../gt.pt')\n",
    "re = torch.load('../re.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attn_output', 'key_states', 'value_states', 'query_states', 'position_ids'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gt['attn_output'], re['attn_output'], rtol=1e-4,atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gt['query_states'], re['query_states'], rtol=1e-3,atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129, 32, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re['key_states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 32, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt['key_states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gt['key_states'][0][128], re['key_states'][0][128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3528,  1.0146,  0.5854,  ...,  0.1339,  0.1257, -0.2622],\n",
       "        [-0.2218, -0.5117,  0.0141,  ...,  0.8779,  0.8389,  0.8540],\n",
       "        [-0.1284,  0.5469,  0.6167,  ...,  1.6211,  0.9756,  0.1910],\n",
       "        ...,\n",
       "        [-1.0947,  0.0994,  0.0122,  ...,  1.2227, -0.4485, -0.0783],\n",
       "        [ 0.1693,  0.2878, -0.2220,  ..., -0.6587,  0.9082, -0.0146],\n",
       "        [ 0.0875,  0.0104,  0.1571,  ...,  0.3452, -0.3425,  0.2004]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt['key_states'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3528,  1.0146,  0.5854,  ...,  0.1339,  0.1257, -0.2622],\n",
       "        [-0.2218, -0.5117,  0.0141,  ...,  0.8779,  0.8389,  0.8540],\n",
       "        [-0.1284,  0.5469,  0.6167,  ...,  1.6211,  0.9756,  0.1910],\n",
       "        ...,\n",
       "        [-1.0947,  0.0994,  0.0122,  ...,  1.2227, -0.4485, -0.0783],\n",
       "        [ 0.1693,  0.2878, -0.2220,  ..., -0.6587,  0.9082, -0.0146],\n",
       "        [ 0.0875,  0.0104,  0.1571,  ...,  0.3452, -0.3425,  0.2004]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re['key_states'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([129, 32, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re['key_states'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head 0 token 128 not found\n",
      "head 0 token 128 not found\n",
      "head 0 done\n",
      "head 1 token 128 not found\n",
      "head 1 token 128 not found\n",
      "head 1 done\n",
      "head 2 token 128 not found\n",
      "head 2 token 128 not found\n",
      "head 2 done\n",
      "head 3 token 128 not found\n",
      "head 3 token 128 not found\n",
      "head 3 done\n",
      "head 4 token 128 not found\n",
      "head 4 token 128 not found\n",
      "head 4 done\n",
      "head 5 token 128 not found\n",
      "head 5 token 128 not found\n",
      "head 5 done\n",
      "head 6 token 128 not found\n",
      "head 6 token 128 not found\n",
      "head 6 done\n",
      "head 7 token 128 not found\n",
      "head 7 token 128 not found\n",
      "head 7 done\n",
      "head 8 token 128 not found\n",
      "head 8 token 128 not found\n",
      "head 8 done\n",
      "head 9 token 128 not found\n",
      "head 9 token 128 not found\n",
      "head 9 done\n",
      "head 10 token 128 not found\n",
      "head 10 token 128 not found\n",
      "head 10 done\n",
      "head 11 token 128 not found\n",
      "head 11 token 128 not found\n",
      "head 11 done\n",
      "head 12 token 128 not found\n",
      "head 12 token 128 not found\n",
      "head 12 done\n",
      "head 13 token 128 not found\n",
      "head 13 token 128 not found\n",
      "head 13 done\n",
      "head 14 token 128 not found\n",
      "head 14 token 128 not found\n",
      "head 14 done\n",
      "head 15 token 128 not found\n",
      "head 15 token 128 not found\n",
      "head 15 done\n",
      "head 16 token 128 not found\n",
      "head 16 token 128 not found\n",
      "head 16 done\n",
      "head 17 token 128 not found\n",
      "head 17 token 128 not found\n",
      "head 17 done\n",
      "head 18 token 128 not found\n",
      "head 18 token 128 not found\n",
      "head 18 done\n",
      "head 19 token 128 not found\n",
      "head 19 token 128 not found\n",
      "head 19 done\n",
      "head 20 token 128 not found\n",
      "head 20 token 128 not found\n",
      "head 20 done\n",
      "head 21 token 128 not found\n",
      "head 21 token 128 not found\n",
      "head 21 done\n",
      "head 22 token 128 not found\n",
      "head 22 token 128 not found\n",
      "head 22 done\n",
      "head 23 token 128 not found\n",
      "head 23 token 128 not found\n",
      "head 23 done\n",
      "head 24 token 128 not found\n",
      "head 24 token 128 not found\n",
      "head 24 done\n",
      "head 25 token 128 not found\n",
      "head 25 token 128 not found\n",
      "head 25 done\n",
      "head 26 token 128 not found\n",
      "head 26 token 128 not found\n",
      "head 26 done\n",
      "head 27 token 128 not found\n",
      "head 27 token 128 not found\n",
      "head 27 done\n",
      "head 28 token 128 not found\n",
      "head 28 token 128 not found\n",
      "head 28 done\n",
      "head 29 token 128 not found\n",
      "head 29 token 128 not found\n",
      "head 29 done\n",
      "head 30 token 128 not found\n",
      "head 30 token 128 not found\n",
      "head 30 done\n",
      "head 31 token 128 not found\n",
      "head 31 token 128 not found\n",
      "head 31 done\n"
     ]
    }
   ],
   "source": [
    "for head in range(32):\n",
    "    for token in range(129):\n",
    "        to_find = gt['key_states'][0][token][head]\n",
    "        found = False\n",
    "        for i in range(129):\n",
    "            if torch.allclose(to_find, re['key_states'][0][i][head]):\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"head {head} token {token} not found\")\n",
    "\n",
    "    for token in range(129):\n",
    "        to_find = gt['value_states'][0][token][head]\n",
    "        found = False\n",
    "        for i in range(129):\n",
    "            if torch.allclose(to_find, re['value_states'][0][i][head]):\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"head {head} token {token} not found\")\n",
    "    print(f\"head {head} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-6.0692e-03, -4.2272e-04,  2.1660e-04,  ..., -6.7062e-03,\n",
       "            3.6836e-04, -9.9640e-03],\n",
       "          [-4.0398e-03, -1.3342e-03,  5.1796e-05,  ..., -2.6207e-03,\n",
       "            9.5177e-04, -7.3195e-04],\n",
       "          [ 9.0504e-04,  2.3041e-03,  5.5962e-03,  ...,  3.0384e-03,\n",
       "            3.1281e-03,  3.0594e-03],\n",
       "          ...,\n",
       "          [-3.2539e-03,  1.1887e-02, -1.7822e-02,  ...,  6.0310e-03,\n",
       "            2.9774e-03, -1.3466e-02],\n",
       "          [-9.0179e-03, -3.5858e-03,  2.0409e-03,  ...,  6.1493e-03,\n",
       "            9.6054e-03,  1.3199e-02],\n",
       "          [-6.7291e-03,  2.5463e-03, -4.2114e-03,  ..., -8.3542e-03,\n",
       "           -9.3651e-04,  4.4365e-03]]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flash_attn import flash_attn_with_kvcache\n",
    "\n",
    "flash_attn_with_kvcache(q=re['query_states'], k_cache=re['key_states'], v_cache=re['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0061, -0.0004,  0.0002,  ..., -0.0084, -0.0009,  0.0044]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt['attn_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gt['attn_output'],flash_attn_with_kvcache(q=re['query_states'], k_cache=re['key_states'], v_cache=re['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True).reshape(1, 1, 4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 128])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_attn_with_kvcache(q=re['query_states'], k_cache=re['key_states'], v_cache=re['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0061, -0.0004,  0.0002,  ..., -0.0084, -0.0009,  0.0044]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_attn_with_kvcache(q=re['query_states'], k_cache=re['key_states'], v_cache=re['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True).reshape(1, 1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[128]], dtype=torch.int32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0061, -0.0004,  0.0002,  ..., -0.0084, -0.0009,  0.0044]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_attn_with_kvcache(q=gt['query_states'], k_cache=gt['key_states'], v_cache=gt['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True, cache_seqlens=torch.tensor([129], dtype=torch.int32).cuda()).reshape(1, 1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0061, -0.0004,  0.0002,  ..., -0.0084, -0.0009,  0.0044]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_attn_with_kvcache(q=re['query_states'], k_cache=re['key_states'], v_cache=re['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True, ).reshape(1, 1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0061, -0.0004,  0.0002,  ..., -0.0084, -0.0009,  0.0044]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_attn_with_kvcache(q=gt['query_states'], k_cache=gt['key_states'], v_cache=gt['value_states'], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True, cache_seqlens=torch.tensor([129], dtype=torch.int32).cuda()).reshape(1, 1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(re['attn_output'], flash_attn_with_kvcache(q=gt['query_states'], k_cache=gt['key_states'][:,:130], v_cache=gt['value_states'][:,:130], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True, cache_seqlens=torch.tensor([129], dtype=torch.int32).cuda()).reshape(1, 1, 4096), rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0061, -0.0004,  0.0002,  ..., -0.0084, -0.0009,  0.0044]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_attn_with_kvcache(q=gt['query_states'], k_cache=gt['key_states'][:,:330], v_cache=gt['value_states'][:,:330], softmax_scale=1/torch.sqrt(torch.tensor(128, dtype=torch.float16)), causal=True, cache_seqlens=torch.tensor([129], dtype=torch.int32).cuda()).reshape(1, 1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 896, 32, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt['key_states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(re['query_states'], gt['query_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 1 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 2 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 3 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 4 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 5 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 6 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 7 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 8 --prefill 122880 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 1 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 2 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 3 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 4 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 5 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 6 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 7 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 8 --prefill 65536 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 1 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 2 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 3 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 4 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 5 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 6 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 7 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 8 --prefill 32768 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 1 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 2 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 3 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 4 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 5 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 6 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 7 --prefill 16384 --attn_method flash_repeat\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 8 --prefill 16384 --attn_method flash_repeat\n"
     ]
    }
   ],
   "source": [
    "bsz_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "prefill_list = [122880, 64*1024, 32*1024, 16*1024]\n",
    "\n",
    "for prefill in prefill_list:\n",
    "    for bsz in bsz_list:\n",
    "        print(f\"CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz {bsz} --prefill {prefill} --attn_method flash_repeat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
