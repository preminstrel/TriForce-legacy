{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2be5e4098d247c684d03efa4e08661c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "import sys\n",
    "root_dir = \"/home/hanshis/workspace/LongContextInfer\"\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import socket\n",
    "\n",
    "from models.modeling_llama import LlamaForCausalLM, LlamaConfig\n",
    "from models.cache_utils import SimpleCache, FlashSimpleCache, GraphFlashSimpleCache, GraphFlashStreamLLMCache\n",
    "from utils.graph_infer import GraphInferenceEngine\n",
    "\n",
    "PREFIX_LEN = 1000\n",
    "T = 100\n",
    "WARM_UP = 10\n",
    "\n",
    "model_name = \"NousResearch/Yarn-Llama-2-7b-128k\"\n",
    "config = LlamaConfig.from_pretrained(model_name)\n",
    "config.flash = True\n",
    "if config.max_position_embeddings < 4096:\n",
    "    config.max_position_embeddings = 1024*128\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "DEC_LEN = 6\n",
    "MAX_LEN = PREFIX_LEN + DEC_LEN\n",
    "\n",
    "cache = FlashSimpleCache(model, MAX_LEN)\n",
    "graph_cache = GraphFlashStreamLLMCache(model, max_budget=1000, prefill=PREFIX_LEN, gamma=DEC_LEN)\n",
    "\n",
    "cache.reset()\n",
    "graph_cache.reset()\n",
    "\n",
    "prefix = torch.randint(low=3, high=30000, size=(1, PREFIX_LEN), device=model.device)\n",
    "assert prefix.shape[-1] == PREFIX_LEN\n",
    "\n",
    "graph_engine = GraphInferenceEngine(model, cache, graph_cache)\n",
    "graph_engine.initialize_cuda_graph([DEC_LEN])\n",
    "\n",
    "# prefill\n",
    "graph_engine.inference(input_ids=prefix)\n",
    "graph_engine.init_graph_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i][:,:16], graph_engine.engine.kv_cache.key_cache[i][:,:16]), f\"{i}\"\n",
    "    assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i][:,16:1000], graph_engine.engine.kv_cache.key_cache[i][:,PREFIX_LEN-984:PREFIX_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22531]], device='cuda:0') tensor([1000], device='cuda:0') tensor([[1000]], device='cuda:0') 0\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[3889]], device='cuda:0') tensor([1001], device='cuda:0') tensor([[1001]], device='cuda:0') 1\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[15555]], device='cuda:0') tensor([1002], device='cuda:0') tensor([[1002]], device='cuda:0') 2\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[26835]], device='cuda:0') tensor([1003], device='cuda:0') tensor([[1003]], device='cuda:0') 3\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[16372]], device='cuda:0') tensor([1004], device='cuda:0') tensor([[1004]], device='cuda:0') 4\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[23916]], device='cuda:0') tensor([1005], device='cuda:0') tensor([[1005]], device='cuda:0') 5\n",
      "Verifying cache consistency, on 1000\n"
     ]
    }
   ],
   "source": [
    "for gamma_offset in range(6):\n",
    "    input_ids = torch.randint(low=3, high=30000, size=(1, 1), device=model.device)\n",
    "    storage_ids = torch.tensor([graph_engine.engine.graph_cache.max_budget + gamma_offset], device=model.device)\n",
    "    position_ids = torch.tensor([graph_engine.engine.graph_cache.max_budget + gamma_offset], device=model.device).unsqueeze(0)\n",
    "    print(input_ids, storage_ids, position_ids, gamma_offset)\n",
    "\n",
    "    # print(graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids, gamma_offset = gamma_offset))\n",
    "    # print(graph_engine.inference(input_ids=input_ids))\n",
    "\n",
    "    print(f\"Verifying cache consistency, on {cache.seq_len}\")\n",
    "    for i in range(32):\n",
    "        assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i], graph_engine.engine.kv_cache.key_cache[i]), f\"{i}\"\n",
    "        assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i], graph_engine.engine.kv_cache.key_cache[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5273e+00,  5.3809e-01, -1.7080e+00,  ..., -1.6025e+00,\n",
       "          -1.6680e+00,  1.4502e+00],\n",
       "         [ 9.7754e-01, -5.9229e-01,  1.3574e+00,  ..., -1.2910e+00,\n",
       "          -1.1514e+00, -8.9111e-01],\n",
       "         [-2.2021e-01, -3.5583e-02,  3.0200e-01,  ...,  1.7744e+00,\n",
       "           1.0371e+00,  1.2500e+00],\n",
       "         ...,\n",
       "         [-2.2675e-02, -4.4098e-02, -2.2217e-01,  ..., -2.6880e-01,\n",
       "          -1.5926e-03, -2.2354e-02],\n",
       "         [-1.1035e+00, -3.0103e-01, -1.7957e-01,  ...,  7.5439e-01,\n",
       "          -9.7021e-01,  3.5596e-01],\n",
       "         [ 1.0293e+00, -9.9426e-02,  2.1631e-01,  ...,  2.1309e+00,\n",
       "          -2.1738e+00,  1.5518e+00]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[1][:,1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e5161a17d4463a82c88629d9cc8cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.5703, -10.8281,  -1.1270,  ..., -11.2422,  -9.7578,  -9.1797],\n",
       "         [-10.6406, -10.2266,   0.2112,  ...,  -6.4961,  -7.9102,  -8.6406],\n",
       "         [-12.8906, -13.6328,  -3.1113,  ...,  -8.7734,  -9.5156,  -8.8984],\n",
       "         ...,\n",
       "         [ -5.1562,  -3.8086,   1.4199,  ...,  -2.3457,  -1.5586,  -2.5527],\n",
       "         [ -5.3203,  -3.6309,   1.7793,  ...,  -1.9453,  -1.2783,  -3.1035],\n",
       "         [ -5.7461,  -5.1328,   1.1377,  ...,  -2.2402,  -1.7432,  -3.1699]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "import sys\n",
    "root_dir = \"/home/hanshis/workspace/LongContextInfer\"\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import socket\n",
    "\n",
    "from models.modeling_llama import LlamaForCausalLM, LlamaConfig\n",
    "from models.cache_utils import SimpleCache, FlashSimpleCache, GraphFlashSimpleCache, GraphFlashStreamLLMCache\n",
    "from utils.graph_infer import GraphInferenceEngine\n",
    "\n",
    "PREFIX_LEN = 1000\n",
    "T = 100\n",
    "WARM_UP = 10\n",
    "\n",
    "model_name = \"NousResearch/Yarn-Llama-2-7b-128k\"\n",
    "config = LlamaConfig.from_pretrained(model_name)\n",
    "config.flash = True\n",
    "if config.max_position_embeddings < 4096:\n",
    "    config.max_position_embeddings = 1024*128\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "DEC_LEN = 6\n",
    "MAX_LEN = PREFIX_LEN + DEC_LEN\n",
    "\n",
    "cache = FlashSimpleCache(model, MAX_LEN)\n",
    "graph_cache = GraphFlashStreamLLMCache(model, max_budget=1000, prefill=PREFIX_LEN, gamma=DEC_LEN)\n",
    "\n",
    "cache.reset()\n",
    "graph_cache.reset()\n",
    "\n",
    "prefix = torch.randint(low=3, high=30000, size=(1, PREFIX_LEN), device=model.device)\n",
    "assert prefix.shape[-1] == PREFIX_LEN\n",
    "\n",
    "graph_engine = GraphInferenceEngine(model, cache, graph_cache)\n",
    "graph_engine.initialize_cuda_graph()\n",
    "\n",
    "# prefill\n",
    "graph_engine.inference(input_ids=prefix)\n",
    "# graph_engine.init_graph_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i][:,:16], graph_engine.engine.kv_cache.key_cache[i][:,:16]), f\"{i}\"\n",
    "    assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i][:,16:1000], graph_engine.engine.kv_cache.key_cache[i][:,PREFIX_LEN-984:PREFIX_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-graph_cache.recent_size + cache.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 1006, 32, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0', size=(32, 0, 1006, 32, 128), dtype=torch.float16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[:][:, -graph_cache.recent_size + cache.seq_len:cache.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[:,:,994,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.key_cache[:,:,1000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1006, 32, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[i][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Budget: 1000  | Real Budget: 1006  | PreFill: 1000  | Start Size: 16  | Recent Size: 984\n",
      "Cached Size: 1001 | Max Budget: 1006\n"
     ]
    }
   ],
   "source": [
    "graph_cache.print_status()\n",
    "cache.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0,0,1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[0][0,1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    assert torch.allclose(graph_cache.key_cache[i][:,1001], cache.key_cache[i][:,1001]), f\"{i}\"\n",
    "    assert torch.allclose(graph_cache.key_cache[i][:,1001], cache.key_cache[i][:,1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.5820, -1.0117,  2.8711,  ..., -1.5615, -1.1182, -1.6680]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids, gamma_offset = gamma_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9414, -1.1523,  2.8789,  ..., -1.5420, -1.2578, -2.1348]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine.inference(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.6484, -3.3320, -0.1942,  ..., -7.1016, -6.2188, -5.1758]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids)\n",
    "\n",
    "graph_cache.update_stream_cache(kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([float('-inf')], dtype=torch.float16, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([inf], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(graph_cache.key_cache[0][0][1000], torch.zeros_like(graph_cache.key_cache[0][0][1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[2][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0194, -0.1855,  0.5117,  ...,  0.3369,  0.0435,  0.4014],\n",
       "        [ 0.9253,  0.4568, -0.7075,  ..., -0.7070,  0.7300, -0.6943],\n",
       "        [-0.5557, -0.6553, -0.3027,  ..., -0.0223, -0.1632, -0.1783],\n",
       "        ...,\n",
       "        [-0.1730,  0.0549, -0.0212,  ..., -0.3787,  0.4285, -0.3359],\n",
       "        [ 0.2054, -0.2064, -2.2402,  ...,  0.4592, -0.2316, -0.2203],\n",
       "        [-1.5889, -1.3701, -1.8203,  ...,  1.6953, -0.5571,  0.4343]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.1914, -4.5703,  2.4707,  ..., -3.9902, -1.9297, -2.4102]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(low=3, high=30000, size=(1, DEC_LEN), device=model.device)\n",
    "storage_ids = torch.arange(DEC_LEN, device=model.device) + PREFIX_LEN\n",
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.graph_infer.GraphInferenceEngine at 0x7fb4f6420af0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.randint(low=3, high=30000, size=(1, DEC_LEN), device=model.device)\n",
    "# storage_ids = torch.arange(DEC_LEN, device=model.device) + PREFIX_LEN\n",
    "# for _ in range(WARM_UP):\n",
    "#     graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids)\n",
    "\n",
    "# cache.print_status()\n",
    "# graph_cache.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import sys\n",
    "root_dir = '/home/hanshis/workspace/LongContextInfer'\n",
    "sys.path.append(root_dir)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from models.modeling_llama_flash import LlamaForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", trust_remote_code=True)\n",
    "model = LlamaForCausalLM.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", revision=\"gptq-4bit-32g-actorder_True\", device_map=\"cuda:9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ee1ab3882f44a6bc8b0386083f52d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1187 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '9'\n",
    "\n",
    "import sys\n",
    "root_dir = '/home/hanshis/workspace/LongContextInfer'\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "# from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from models.llama_gptq import LlamaGPTQForCausalLM\n",
    "\n",
    "\n",
    "model = LlamaGPTQForCausalLM.from_quantized(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def rerange_kv_cache(kv_cache, chunk_size):\n",
    "\n",
    "    num_clusters = kv_cache.seq_len // chunk_size\n",
    "    assert num_clusters * chunk_size == kv_cache.seq_len, \"max_budget should be divisible by chunk_size\"\n",
    "\n",
    "    for layer in range(kv_cache.layers):\n",
    "        for head_index in range(kv_cache.num_heads):\n",
    "            # (bsz, max_budget, head_dim) --> (bsz * max_budget, head_dim)\n",
    "            head_key_cache = kv_cache.key_cache[layer][:, :, head_index, :].reshape(-1, kv_cache.head_dim).numpy() \n",
    "            head_value_cache = kv_cache.value_cache[layer][:, :, head_index, :].reshape(-1, kv_cache.head_dim).numpy()\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=head_index).fit(head_key_cache)\n",
    "            \n",
    "            labels = kmeans.labels_\n",
    "            sorted_indices = np.argsort(labels)\n",
    "            sorted_head_key = head_key_cache[sorted_indices]\n",
    "            sorted_head_value = head_value_cache[sorted_indices]\n",
    "\n",
    "        \n",
    "            kv_cache.key_cache[layer][:, :, head_index, :] = torch.tensor(sorted_head_key).reshape(1, kv_cache.seq_len, kv_cache.head_dim)\n",
    "            kv_cache.value_cache[layer][:, :, head_index, :] = torch.tensor(sorted_head_value).reshape(1, kv_cache.seq_len, kv_cache.head_dim)\n",
    "    print(\"Rerange KV cache complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终的KV缓存形状: torch.Size([1, 128, 8, 64]) torch.Size([1, 128, 8, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 设置参数\n",
    "num_heads = 8\n",
    "max_budget = 8 * 16  # 每个头有16个KV对，总共8个头\n",
    "head_dim = 64\n",
    "num_clusters = 8  # 总共8个聚类，每个聚类对应于一个头的16个KV对\n",
    "chunk_size = 16  # 每个聚类包含16个KV对\n",
    "\n",
    "# 模拟的KV缓存数据\n",
    "key_cache = torch.rand(1, max_budget, num_heads, head_dim)\n",
    "value_cache = torch.rand(1, max_budget, num_heads, head_dim)\n",
    "\n",
    "# 初始化一个空的tensor用于存放重排列后的KV缓存\n",
    "clustered_key_cache = torch.empty(1, max_budget, num_heads, head_dim)\n",
    "clustered_value_cache = torch.empty(1, max_budget, num_heads, head_dim)\n",
    "\n",
    "# 对每个头独立进行聚类\n",
    "for head_index in range(num_heads):\n",
    "    # 提取当前头的KV缓存\n",
    "    head_key_cache = key_cache[:, :, head_index, :].reshape(-1, head_dim).numpy() # (bsz, max_budget, head_dim) --> (bsz * max_budget, head_dim)\n",
    "    head_value_cache = value_cache[:, :, head_index, :].reshape(-1, head_dim).numpy()\n",
    "    \n",
    "    # 执行KMeans聚类\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=head_index).fit(head_key_cache)\n",
    "    \n",
    "    # 获取聚类标签并根据这些标签排序KV缓存\n",
    "    labels = kmeans.labels_\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_head_key = head_key_cache[sorted_indices]\n",
    "    sorted_head_value = head_value_cache[sorted_indices]\n",
    "\n",
    "    \n",
    "    # 将排序后的KV缓存重新放入对应的头中\n",
    "    # print(torch.tensor(sorted_head_kv).shape, clustered_kv_cache[:, :, head_index, :].shape)\n",
    "    clustered_key_cache[:, :, head_index, :] = torch.tensor(sorted_head_key).reshape(1, max_budget, head_dim)\n",
    "    clustered_value_cache[:, :, head_index, :] = torch.tensor(sorted_head_value).reshape(1, max_budget, head_dim)\n",
    "\n",
    "# 验证最终形状\n",
    "print(\"最终的KV缓存形状:\", clustered_key_cache.shape, clustered_value_cache.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始KV缓存中的元素是否与重排列后的KV缓存中的完全相同？ True\n"
     ]
    }
   ],
   "source": [
    "# 假设 kv_cache 是原始的KV缓存，clustered_kv_cache 是重排列后的KV缓存\n",
    "\n",
    "# 扁平化并排序两个KV缓存\n",
    "original_flat_sorted = torch.sort(key_cache.reshape(-1))[0]\n",
    "new_flat_sorted = torch.sort(clustered_key_cache.reshape(-1))[0]\n",
    "\n",
    "# 比较两个排序后的扁平化KV缓存是否完全相同\n",
    "are_elements_identical = torch.all(original_flat_sorted == new_flat_sorted)\n",
    "\n",
    "original_flat_sorted = torch.sort(value_cache.reshape(-1))[0]\n",
    "new_flat_sorted = torch.sort(clustered_value_cache.reshape(-1))[0]\n",
    "\n",
    "# 比较两个排序后的扁平化KV缓存是否完全相同\n",
    "are_elements_identical = torch.all(original_flat_sorted == new_flat_sorted)\n",
    "\n",
    "print(\"原始KV缓存中的元素是否与重排列后的KV缓存中的完全相同？\", are_elements_identical.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements in head 0 are correctly rearranged.\n",
      "All elements in head 1 are correctly rearranged.\n",
      "All elements in head 2 are correctly rearranged.\n",
      "All elements in head 3 are correctly rearranged.\n",
      "All elements in head 4 are correctly rearranged.\n",
      "All elements in head 5 are correctly rearranged.\n",
      "All elements in head 6 are correctly rearranged.\n",
      "All elements in head 7 are correctly rearranged.\n",
      "All elements in head 0 are correctly rearranged.\n",
      "All elements in head 1 are correctly rearranged.\n",
      "All elements in head 2 are correctly rearranged.\n",
      "All elements in head 3 are correctly rearranged.\n",
      "All elements in head 4 are correctly rearranged.\n",
      "All elements in head 5 are correctly rearranged.\n",
      "All elements in head 6 are correctly rearranged.\n",
      "All elements in head 7 are correctly rearranged.\n"
     ]
    }
   ],
   "source": [
    "original_key_cache = key_cache  # 使用之前示例中的模拟数据\n",
    "original_value_cache = value_cache  # 使用之前示例中的模拟数据\n",
    "\n",
    "# 假设 rearranged_kv_cache 是重排列后的KV缓存，其形状为 [1, 128, num_heads, head_dim]\n",
    "rearranged_key_cache = clustered_key_cache \n",
    "\n",
    "for head in range(num_heads):\n",
    "    original_head_data = original_key_cache[:, :, head, :].reshape(-1, head_dim)\n",
    "    rearranged_head_data = rearranged_key_cache[:, :, head, :].reshape(-1, head_dim)\n",
    "    \n",
    "    # 因为重排列改变了元素的顺序，我们不能直接比较对应位置的元素\n",
    "    # 相 stat代，我们检查重排列后的每个元素是否在原始数据中\n",
    "    found_all_elements = True\n",
    "    for elem in rearranged_head_data:\n",
    "        if not any(torch.allclose(elem, original_elem) for original_elem in original_head_data):\n",
    "            found_all_elements = False\n",
    "            break\n",
    "    \n",
    "    if found_all_elements:\n",
    "        print(f\"All elements in head {head} are correctly rearranged.\")\n",
    "    else:\n",
    "        print(f\"Not all elements in head {head} are found in the rearranged KV cache.\")\n",
    "\n",
    "original_key_cache = key_cache  # 使用之前示例中的模拟数据\n",
    "\n",
    "# 假设 rearranged_kv_cache 是重排列后的KV缓存，其形状为 [1, 128, num_heads, head_dim]\n",
    "rearranged_value_cache = clustered_value_cache \n",
    "\n",
    "for head in range(num_heads):\n",
    "    original_head_data = original_value_cache[:, :, head, :].reshape(-1, head_dim)\n",
    "    rearranged_head_data = rearranged_value_cache[:, :, head, :].reshape(-1, head_dim)\n",
    "    \n",
    "    # 因为重排列改变了元素的顺序，我们不能直接比较对应位置的元素\n",
    "    # 相 stat代，我们检查重排列后的每个元素是否在原始数据中\n",
    "    found_all_elements = True\n",
    "    for elem in rearranged_head_data:\n",
    "        if not any(torch.allclose(elem, original_elem) for original_elem in original_head_data):\n",
    "            found_all_elements = False\n",
    "            break\n",
    "    \n",
    "    if found_all_elements:\n",
    "        print(f\"All elements in head {head} are correctly rearranged.\")\n",
    "    else:\n",
    "        print(f\"Not all elements in head {head} are found in the rearranged KV cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def capture_graph(mempool=None, n_warmups :int=3):\n",
    "    device = \"cuda:0\"\n",
    "    # draft run is incremental decoding\n",
    "    static_position_ids = torch.tensor([[100]], device=device)\n",
    "    \n",
    "    s = torch.cuda.Stream()\n",
    "    s.wait_stream(torch.cuda.current_stream())\n",
    "    with torch.cuda.stream(s):\n",
    "        for _ in range(n_warmups):\n",
    "            out = torch.arange(0, static_position_ids[0, 0] + 1, device=device)\n",
    "        s.synchronize()\n",
    "    torch.cuda.current_stream().wait_stream(s)\n",
    "\n",
    "    graph = torch.cuda.CUDAGraph()\n",
    "    with torch.cuda.graph(graph, pool=mempool):\n",
    "        out = torch.arange(0, static_position_ids[0, 0] + 1, device=device)\n",
    "    \n",
    "    def run(position_ids):\n",
    "        static_position_ids.copy_(position_ids)\n",
    "        graph.replay()\n",
    "        return out.clone()\n",
    "\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
