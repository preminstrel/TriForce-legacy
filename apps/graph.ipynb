{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cabbe1e333498eb3008b6e31382eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached Size: 1000 | Max Budget: 1512\n",
      "Max Budget: 1512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "import sys\n",
    "root_dir = \"/home/hanshis/workspace/LongContextInfer\"\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import socket\n",
    "\n",
    "from models.modeling_llama import LlamaForCausalLM, LlamaConfig\n",
    "from models.cache_utils import SimpleCache, FlashSimpleCache, GraphFlashSimpleCache\n",
    "from utils.graph_infer import GraphInferenceEngine\n",
    "\n",
    "PREFIX_LEN = 1000\n",
    "T = 100\n",
    "WARM_UP = 10\n",
    "\n",
    "host = socket.gethostname()\n",
    "if 'lovelace' in host:\n",
    "    file_path = \"/home/hanshis/workspace/LongContextInfer/benchmark/report/L40_llama_7B_128K_graph.csv\"\n",
    "else:\n",
    "    file_path = \"/fsx-storygen/beidic/hanshi/LongContextInfer/benchmark/report/A100_llama_7B_128K_graph.csv\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        contents = f.read()\n",
    "except FileNotFoundError:\n",
    "    contents = \"\"\n",
    "\n",
    "if not contents:\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(\"model,prefill,len,latency,repeat_time,flash\\n\")\n",
    "\n",
    "model_name = \"NousResearch/Yarn-Llama-2-7b-128k\"\n",
    "config = LlamaConfig.from_pretrained(model_name)\n",
    "config.flash = True\n",
    "if config.max_position_embeddings < 4096:\n",
    "    config.max_position_embeddings = 1024*128\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# DEC_LEN_LIST = [1,2,4,8,16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,272,288,304,320,336,352,368,384,400,416,432,448,464,480,496,512]\n",
    "\n",
    "DEC_LEN_LIST = [1]\n",
    "\n",
    "MAX_LEN = PREFIX_LEN + 512\n",
    "\n",
    "cache = FlashSimpleCache(model, MAX_LEN)\n",
    "graph_cache = GraphFlashSimpleCache(model, MAX_LEN)\n",
    "\n",
    "for DEC_LEN in DEC_LEN_LIST:\n",
    "    cache.reset()\n",
    "    graph_cache.reset()\n",
    "    prefix = torch.randint(low=3, high=30000, size=(1, PREFIX_LEN), device=model.device)\n",
    "    assert prefix.shape[-1] == PREFIX_LEN\n",
    "\n",
    "    graph_engine = GraphInferenceEngine(model, cache, graph_cache)\n",
    "    graph_engine.initialize_cuda_graph([DEC_LEN])\n",
    "\n",
    "    graph_engine.inference(input_ids=prefix)\n",
    "\n",
    "    cache.print_status()\n",
    "    graph_cache.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[1][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(graph_cache.key_cache[0][0][1000], torch.zeros_like(graph_cache.key_cache[0][0][1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[2][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0194, -0.1855,  0.5117,  ...,  0.3369,  0.0435,  0.4014],\n",
       "        [ 0.9253,  0.4568, -0.7075,  ..., -0.7070,  0.7300, -0.6943],\n",
       "        [-0.5557, -0.6553, -0.3027,  ..., -0.0223, -0.1632, -0.1783],\n",
       "        ...,\n",
       "        [-0.1730,  0.0549, -0.0212,  ..., -0.3787,  0.4285, -0.3359],\n",
       "        [ 0.2054, -0.2064, -2.2402,  ...,  0.4592, -0.2316, -0.2203],\n",
       "        [-1.5889, -1.3701, -1.8203,  ...,  1.6953, -0.5571,  0.4343]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.1914, -4.5703,  2.4707,  ..., -3.9902, -1.9297, -2.4102]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(low=3, high=30000, size=(1, DEC_LEN), device=model.device)\n",
    "storage_ids = torch.arange(DEC_LEN, device=model.device) + PREFIX_LEN\n",
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.graph_infer.GraphInferenceEngine at 0x7fb4f6420af0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.randint(low=3, high=30000, size=(1, DEC_LEN), device=model.device)\n",
    "# storage_ids = torch.arange(DEC_LEN, device=model.device) + PREFIX_LEN\n",
    "# for _ in range(WARM_UP):\n",
    "#     graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids)\n",
    "\n",
    "# cache.print_status()\n",
    "# graph_cache.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import sys\n",
    "root_dir = '/home/hanshis/workspace/LongContextInfer'\n",
    "sys.path.append(root_dir)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from models.modeling_llama_flash import LlamaForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", trust_remote_code=True)\n",
    "model = LlamaForCausalLM.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", revision=\"gptq-4bit-32g-actorder_True\", device_map=\"cuda:9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ee1ab3882f44a6bc8b0386083f52d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1187 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '9'\n",
    "\n",
    "import sys\n",
    "root_dir = '/home/hanshis/workspace/LongContextInfer'\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "# from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from models.llama_gptq import LlamaGPTQForCausalLM\n",
    "\n",
    "\n",
    "model = LlamaGPTQForCausalLM.from_quantized(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
