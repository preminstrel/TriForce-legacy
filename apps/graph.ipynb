{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2be5e4098d247c684d03efa4e08661c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n",
      "capturing graph...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "import sys\n",
    "root_dir = \"/home/hanshis/workspace/LongContextInfer\"\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import socket\n",
    "\n",
    "from models.modeling_llama import LlamaForCausalLM, LlamaConfig\n",
    "from models.cache_utils import SimpleCache, FlashSimpleCache, GraphFlashSimpleCache, GraphFlashStreamLLMCache\n",
    "from utils.graph_infer import GraphInferenceEngine\n",
    "\n",
    "PREFIX_LEN = 1000\n",
    "T = 100\n",
    "WARM_UP = 10\n",
    "\n",
    "model_name = \"NousResearch/Yarn-Llama-2-7b-128k\"\n",
    "config = LlamaConfig.from_pretrained(model_name)\n",
    "config.flash = True\n",
    "if config.max_position_embeddings < 4096:\n",
    "    config.max_position_embeddings = 1024*128\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "DEC_LEN = 6\n",
    "MAX_LEN = PREFIX_LEN + DEC_LEN\n",
    "\n",
    "cache = FlashSimpleCache(model, MAX_LEN)\n",
    "graph_cache = GraphFlashStreamLLMCache(model, max_budget=1000, prefill=PREFIX_LEN, gamma=DEC_LEN)\n",
    "\n",
    "cache.reset()\n",
    "graph_cache.reset()\n",
    "\n",
    "prefix = torch.randint(low=3, high=30000, size=(1, PREFIX_LEN), device=model.device)\n",
    "assert prefix.shape[-1] == PREFIX_LEN\n",
    "\n",
    "graph_engine = GraphInferenceEngine(model, cache, graph_cache)\n",
    "graph_engine.initialize_cuda_graph([DEC_LEN])\n",
    "\n",
    "# prefill\n",
    "graph_engine.inference(input_ids=prefix)\n",
    "graph_engine.init_graph_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i][:,:16], graph_engine.engine.kv_cache.key_cache[i][:,:16]), f\"{i}\"\n",
    "    assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i][:,16:1000], graph_engine.engine.kv_cache.key_cache[i][:,PREFIX_LEN-984:PREFIX_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22531]], device='cuda:0') tensor([1000], device='cuda:0') tensor([[1000]], device='cuda:0') 0\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[3889]], device='cuda:0') tensor([1001], device='cuda:0') tensor([[1001]], device='cuda:0') 1\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[15555]], device='cuda:0') tensor([1002], device='cuda:0') tensor([[1002]], device='cuda:0') 2\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[26835]], device='cuda:0') tensor([1003], device='cuda:0') tensor([[1003]], device='cuda:0') 3\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[16372]], device='cuda:0') tensor([1004], device='cuda:0') tensor([[1004]], device='cuda:0') 4\n",
      "Verifying cache consistency, on 1000\n",
      "tensor([[23916]], device='cuda:0') tensor([1005], device='cuda:0') tensor([[1005]], device='cuda:0') 5\n",
      "Verifying cache consistency, on 1000\n"
     ]
    }
   ],
   "source": [
    "for gamma_offset in range(6):\n",
    "    input_ids = torch.randint(low=3, high=30000, size=(1, 1), device=model.device)\n",
    "    storage_ids = torch.tensor([graph_engine.engine.graph_cache.max_budget + gamma_offset], device=model.device)\n",
    "    position_ids = torch.tensor([graph_engine.engine.graph_cache.max_budget + gamma_offset], device=model.device).unsqueeze(0)\n",
    "    print(input_ids, storage_ids, position_ids, gamma_offset)\n",
    "\n",
    "    # print(graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids, gamma_offset = gamma_offset))\n",
    "    # print(graph_engine.inference(input_ids=input_ids))\n",
    "\n",
    "    print(f\"Verifying cache consistency, on {cache.seq_len}\")\n",
    "    for i in range(32):\n",
    "        assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i], graph_engine.engine.kv_cache.key_cache[i]), f\"{i}\"\n",
    "        assert torch.allclose(graph_engine.engine.graph_cache.key_cache[i], graph_engine.engine.kv_cache.key_cache[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5273e+00,  5.3809e-01, -1.7080e+00,  ..., -1.6025e+00,\n",
       "          -1.6680e+00,  1.4502e+00],\n",
       "         [ 9.7754e-01, -5.9229e-01,  1.3574e+00,  ..., -1.2910e+00,\n",
       "          -1.1514e+00, -8.9111e-01],\n",
       "         [-2.2021e-01, -3.5583e-02,  3.0200e-01,  ...,  1.7744e+00,\n",
       "           1.0371e+00,  1.2500e+00],\n",
       "         ...,\n",
       "         [-2.2675e-02, -4.4098e-02, -2.2217e-01,  ..., -2.6880e-01,\n",
       "          -1.5926e-03, -2.2354e-02],\n",
       "         [-1.1035e+00, -3.0103e-01, -1.7957e-01,  ...,  7.5439e-01,\n",
       "          -9.7021e-01,  3.5596e-01],\n",
       "         [ 1.0293e+00, -9.9426e-02,  2.1631e-01,  ...,  2.1309e+00,\n",
       "          -2.1738e+00,  1.5518e+00]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[1][:,1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cache.update_stream_cache(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-graph_cache.recent_size + cache.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 1006, 32, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0', size=(32, 0, 1006, 32, 128), dtype=torch.float16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[:][:, -graph_cache.recent_size + cache.seq_len:cache.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[:,:,994,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.key_cache[:,:,1000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1006, 32, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[i][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Budget: 1000  | Real Budget: 1006  | PreFill: 1000  | Start Size: 16  | Recent Size: 984\n",
      "Cached Size: 1001 | Max Budget: 1006\n"
     ]
    }
   ],
   "source": [
    "graph_cache.print_status()\n",
    "cache.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0,0,1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[0][0,1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    assert torch.allclose(graph_cache.key_cache[i][:,1001], cache.key_cache[i][:,1001]), f\"{i}\"\n",
    "    assert torch.allclose(graph_cache.key_cache[i][:,1001], cache.key_cache[i][:,1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.5820, -1.0117,  2.8711,  ..., -1.5615, -1.1182, -1.6680]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids, gamma_offset = gamma_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9414, -1.1523,  2.8789,  ..., -1.5420, -1.2578, -2.1348]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine.inference(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.6484, -3.3320, -0.1942,  ..., -7.1016, -6.2188, -5.1758]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids, position_ids=position_ids)\n",
    "\n",
    "graph_cache.update_stream_cache(kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([float('-inf')], dtype=torch.float16, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([inf], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(graph_cache.key_cache[0][0][1000], torch.zeros_like(graph_cache.key_cache[0][0][1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[2][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0194, -0.1855,  0.5117,  ...,  0.3369,  0.0435,  0.4014],\n",
       "        [ 0.9253,  0.4568, -0.7075,  ..., -0.7070,  0.7300, -0.6943],\n",
       "        [-0.5557, -0.6553, -0.3027,  ..., -0.0223, -0.1632, -0.1783],\n",
       "        ...,\n",
       "        [-0.1730,  0.0549, -0.0212,  ..., -0.3787,  0.4285, -0.3359],\n",
       "        [ 0.2054, -0.2064, -2.2402,  ...,  0.4592, -0.2316, -0.2203],\n",
       "        [-1.5889, -1.3701, -1.8203,  ...,  1.6953, -0.5571,  0.4343]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_cache.key_cache[0][0][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.1914, -4.5703,  2.4707,  ..., -3.9902, -1.9297, -2.4102]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(low=3, high=30000, size=(1, DEC_LEN), device=model.device)\n",
    "storage_ids = torch.arange(DEC_LEN, device=model.device) + PREFIX_LEN\n",
    "graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.graph_infer.GraphInferenceEngine at 0x7fb4f6420af0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.randint(low=3, high=30000, size=(1, DEC_LEN), device=model.device)\n",
    "# storage_ids = torch.arange(DEC_LEN, device=model.device) + PREFIX_LEN\n",
    "# for _ in range(WARM_UP):\n",
    "#     graph_engine.graph_inference(input_ids=input_ids, storage_ids=storage_ids)\n",
    "\n",
    "# cache.print_status()\n",
    "# graph_cache.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import sys\n",
    "root_dir = '/home/hanshis/workspace/LongContextInfer'\n",
    "sys.path.append(root_dir)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from models.modeling_llama_flash import LlamaForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", trust_remote_code=True)\n",
    "model = LlamaForCausalLM.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", revision=\"gptq-4bit-32g-actorder_True\", device_map=\"cuda:9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ee1ab3882f44a6bc8b0386083f52d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1187 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '9'\n",
    "\n",
    "import sys\n",
    "root_dir = '/home/hanshis/workspace/LongContextInfer'\n",
    "sys.path.append(root_dir)\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "# from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from models.llama_gptq import LlamaGPTQForCausalLM\n",
    "\n",
    "\n",
    "model = LlamaGPTQForCausalLM.from_quantized(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
