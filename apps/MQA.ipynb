{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a720b6f38f94599b5635781458b3d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/hanshis/workspace/LongContextInfer')\n",
    "\n",
    "import torch\n",
    "from models.modeling_llama import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model = \"NousResearch/Yarn-Llama-2-7b-128k\"\n",
    "model = \"01-ai/Yi-6B-200K\"\n",
    "\n",
    "target = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16, device_map=\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.model.layers[0].self_attn.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " "
     ]
    }
   ],
   "source": [
    "from models.cache_utils import FlashSimpleCache\n",
    "\n",
    "with torch.inference_mode():\n",
    "    cache = FlashSimpleCache(target, 1024)\n",
    "    prompts = \"Hello, my dog is cute\"\n",
    "    next_token = tokenizer.encode(prompts, return_tensors=\"pt\").to(target.device)\n",
    "    for i in range(100):\n",
    "        if next_token.shape == torch.Size([1]):\n",
    "            next_token = next_token.unsqueeze(0)\n",
    "        if next_token.shape == torch.Size([]):\n",
    "            next_token = next_token.unsqueeze(0).unsqueeze(0)\n",
    "        logits = target(input_ids=next_token, kv_cache=cache, graph_cache=None).logits\n",
    "        next_token = torch.argmax(logits[:, -1, :])\n",
    "        print(tokenizer.decode(next_token), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e25677a836b4cdf844a90ff6e9ba7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys, torch\n",
    "sys.path.append('/home/hanshis/workspace/LongContextInfer')\n",
    "\n",
    "from models.modeling_batch_llama import LlamaForCausalLM\n",
    "from models.batch_cache import BatchSimpleCache\n",
    "from transformers import AutoTokenizer\n",
    "model = \"01-ai/Yi-6B-200K\"\n",
    "target_bsz = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16, device_map=\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    prompts = \"Hello, my dog is cute\"\n",
    "    next_token = tokenizer.encode(prompts, return_tensors=\"pt\").to(target_bsz.device)\n",
    "    cache_bsz = BatchSimpleCache(target_bsz, 1024, 1)\n",
    "    n=0\n",
    "    cache_bsz.reset()\n",
    "    gen_tokens = torch.zeros((1, 100), dtype=torch.long, device=target_bsz.device)\n",
    "    while n < 100:\n",
    "        logits = target_bsz(input_ids=next_token, kv_cache=cache_bsz, graph_cache=None).logits\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        gen_tokens[:, n] = next_token.squeeze()\n",
    "        next_token = next_token.unsqueeze(1)\n",
    "        n += 1\n",
    "    tokenizer.batch_decode(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(gen_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(target.model.layers[1].self_attn.rotary_emb.cos_cached, target_bsz.model.layers[1].self_attn.rotary_emb.cos_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[0,:,:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(cache.key_cache[0,:,:1], cache_bsz.key_cache[0,:,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(cache.key_cache[1,:,:1], cache_bsz.key_cache[1,:,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_models_identical(model1, model2):\n",
    "    # 检查模型参数\n",
    "    params1 = list(model1.parameters())\n",
    "    params2 = list(model2.parameters())\n",
    "    if len(params1) != len(params2):\n",
    "        print(\"false\")\n",
    "        return False\n",
    "    for p1, p2 in zip(params1, params2):\n",
    "        if p1.data.ne(p2.data).sum() > 0:  # 检查是否有不同的元素\n",
    "            print(\"false\")\n",
    "            return False\n",
    "    \n",
    "    # 检查模型的缓冲区\n",
    "    buffers1 = list(model1.buffers())\n",
    "    buffers2 = list(model2.buffers())\n",
    "    if len(buffers1) != len(buffers2):\n",
    "        print(\"false\")\n",
    "        return False\n",
    "    for b1, b2 in zip(buffers1, buffers2):\n",
    "        if b1.data.ne(b2.data).sum() > 0:  # 检查是否有不同的元素\n",
    "            print(\"false\")\n",
    "            return False\n",
    "    \n",
    "    # 如果所有检查都通过了，则模型是完全相同的\n",
    "    return True\n",
    "\n",
    "check_models_identical(target, target_bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 2 --prefill 491520\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 4 --prefill 245760\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 6 --prefill 163840\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 8 --prefill 122880\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 10 --prefill 98304\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 12 --prefill 81920\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 14 --prefill 70217\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 16 --prefill 61440\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 18 --prefill 54613\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 20 --prefill 49152\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 22 --prefill 44683\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 24 --prefill 40960\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 26 --prefill 37809\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 28 --prefill 35108\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 30 --prefill 32768\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 32 --prefill 30720\n"
     ]
    }
   ],
   "source": [
    "for i in [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]:\n",
    "    print(f\"CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz {i} --prefill {122880*8//i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_with_kvcache\n",
    "from transformers.models.llama.modeling_llama import(\n",
    "    repeat_kv,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "head_dim = 128\n",
    "\n",
    "def benchmark_mqa_attn(attn_method, query_states, key_states, value_states, num_key_value_groups, seq_len):\n",
    "    if attn_method == 'flash':\n",
    "        time1 = time.time()\n",
    "        for i in range(1000):\n",
    "            attn_output = flash_attn_with_kvcache(q=query_states, k_cache=key_states, v_cache=value_states, cache_seqlens=seq_len, softmax_scale=1/torch.sqrt(torch.tensor(head_dim, dtype=torch.float16)), causal=True)\n",
    "        latency = (time.time()-time1)\n",
    "    \n",
    "    elif attn_method == 'flash_repeat':\n",
    "        time1 = time.time()\n",
    "        for i in range(1000):\n",
    "            key_states = repeat_kv(key_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            value_states = repeat_kv(value_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            attn_output = flash_attn_with_kvcache(q=query_states, k_cache=key_states, v_cache=value_states, cache_seqlens=seq_len, softmax_scale=1/torch.sqrt(torch.tensor(head_dim, dtype=torch.float16)), causal=True)\n",
    "        latency = (time.time()-time1)\n",
    "\n",
    "    elif attn_method == 'sdpa':\n",
    "        time1 = time.time()\n",
    "        for i in range(1000):\n",
    "            key_states = repeat_kv(key_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            value_states = repeat_kv(value_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            with torch.backends.cuda.sdp_kernel(enable_math=False):\n",
    "                attn_output = F.scaled_dot_product_attention(query_states.transpose(1,2),key_states.transpose(1,2),value_states.transpose(1,2), is_causal=True)\n",
    "        latency = (time.time()-time1)\n",
    "    \n",
    "    elif attn_method == 'vanilla':\n",
    "        time1 = time.time()\n",
    "        for i in range(1000):\n",
    "            key_states = repeat_kv(key_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            value_states = repeat_kv(value_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            query_states= query_states.transpose(1,2)\n",
    "            key_states= key_states.transpose(1,2)\n",
    "            value_states= value_states.transpose(1,2)\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        latency = (time.time()-time1)\n",
    "\n",
    "    return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsz: 1, prefill: [65536], input: 1, flash: 0.33137381076812744\n",
      "bsz: 1, prefill: [65536], input: 1, vanilla: 4.519198656082153\n",
      "bsz: 1, prefill: [65536], input: 1, optim: 0.9695037603378296\n",
      "bsz: 1, prefill: [65536], input: 1, flash-ref: 0.3322429656982422\n",
      "bsz: 1, prefill: [65536], input: 1, vanilla-ref: 0.587114691734314\n",
      "=======================================================================\n",
      "bsz: 1, prefill: [65536], input: 2, flash: 0.7587827444076538\n",
      "bsz: 1, prefill: [65536], input: 2, vanilla: 3.9973089694976807\n",
      "bsz: 1, prefill: [65536], input: 2, optim: 0.750908374786377\n",
      "bsz: 1, prefill: [65536], input: 2, flash-ref: 0.33817434310913086\n",
      "bsz: 1, prefill: [65536], input: 2, vanilla-ref: 0.7119020223617554\n",
      "=======================================================================\n",
      "bsz: 1, prefill: [65536], input: 4, flash: 0.760932207107544\n",
      "bsz: 1, prefill: [65536], input: 4, vanilla: 4.250280380249023\n",
      "bsz: 1, prefill: [65536], input: 4, optim: 0.8241993188858032\n",
      "bsz: 1, prefill: [65536], input: 4, flash-ref: 0.3522707223892212\n",
      "bsz: 1, prefill: [65536], input: 4, vanilla-ref: 0.7200660705566406\n",
      "=======================================================================\n",
      "bsz: 1, prefill: [65536], input: 8, flash: 0.7724212408065796\n",
      "bsz: 1, prefill: [65536], input: 8, vanilla: 4.759918928146362\n",
      "bsz: 1, prefill: [65536], input: 8, optim: 0.9779887199401855\n",
      "bsz: 1, prefill: [65536], input: 8, flash-ref: 0.3624575138092041\n",
      "bsz: 1, prefill: [65536], input: 8, vanilla-ref: 0.6375143527984619\n",
      "=======================================================================\n",
      "bsz: 1, prefill: [65536], input: 16, flash: 0.7993183135986328\n",
      "bsz: 1, prefill: [65536], input: 16, vanilla: 4.673551559448242\n",
      "bsz: 1, prefill: [65536], input: 16, optim: 1.6643104553222656\n",
      "bsz: 1, prefill: [65536], input: 16, flash-ref: 0.35288679599761963\n",
      "bsz: 1, prefill: [65536], input: 16, vanilla-ref: 0.6533118486404419\n",
      "=======================================================================\n",
      "***********************************************************************\n",
      "bsz: 2, prefill: [65536 65536], input: 1, flash: 0.8347214460372925\n",
      "bsz: 2, prefill: [65536 65536], input: 1, vanilla: 8.1726815700531\n",
      "bsz: 2, prefill: [65536 65536], input: 1, optim: 2.659473419189453\n",
      "bsz: 2, prefill: [65536 65536], input: 1, flash-ref: 0.8320752382278442\n",
      "bsz: 2, prefill: [65536 65536], input: 1, vanilla-ref: 2.132112503051758\n",
      "=======================================================================\n",
      "bsz: 2, prefill: [65536 65536], input: 2, flash: 1.6375043392181396\n",
      "bsz: 2, prefill: [65536 65536], input: 2, vanilla: 8.406714677810669\n",
      "bsz: 2, prefill: [65536 65536], input: 2, optim: 2.4830148220062256\n",
      "bsz: 2, prefill: [65536 65536], input: 2, flash-ref: 0.8328834772109985\n",
      "bsz: 2, prefill: [65536 65536], input: 2, vanilla-ref: 2.351408362388611\n",
      "=======================================================================\n",
      "bsz: 2, prefill: [65536 65536], input: 4, flash: 1.6471571922302246\n",
      "bsz: 2, prefill: [65536 65536], input: 4, vanilla: 8.99484372138977\n",
      "bsz: 2, prefill: [65536 65536], input: 4, optim: 2.77576220035553\n",
      "bsz: 2, prefill: [65536 65536], input: 4, flash-ref: 0.835209846496582\n",
      "bsz: 2, prefill: [65536 65536], input: 4, vanilla-ref: 2.388118863105774\n",
      "=======================================================================\n",
      "bsz: 2, prefill: [65536 65536], input: 8, flash: 1.6658356189727783\n",
      "bsz: 2, prefill: [65536 65536], input: 8, vanilla: 9.995793223381042\n",
      "bsz: 2, prefill: [65536 65536], input: 8, optim: 3.2607064247131348\n",
      "bsz: 2, prefill: [65536 65536], input: 8, flash-ref: 0.8336025476455688\n",
      "bsz: 2, prefill: [65536 65536], input: 8, vanilla-ref: 2.48827862739563\n",
      "=======================================================================\n",
      "bsz: 2, prefill: [65536 65536], input: 16, flash: 1.6947160959243774\n",
      "bsz: 2, prefill: [65536 65536], input: 16, vanilla: 9.533504962921143\n",
      "bsz: 2, prefill: [65536 65536], input: 16, optim: 4.595667362213135\n",
      "bsz: 2, prefill: [65536 65536], input: 16, flash-ref: 0.8359875679016113\n",
      "bsz: 2, prefill: [65536 65536], input: 16, vanilla-ref: 2.5664985179901123\n",
      "=======================================================================\n",
      "***********************************************************************\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 1, flash: 1.7921708822250366\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 1, vanilla: 17.71196734905243\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 1, optim: 4.884820818901062\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 1, flash-ref: 1.7905733585357666\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 1, vanilla-ref: 4.377990007400513\n",
      "=======================================================================\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 2, flash: 3.3809592723846436\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 2, vanilla: 16.182042121887207\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 2, optim: 4.892948746681213\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 2, flash-ref: 1.7969818115234375\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 2, vanilla-ref: 4.570010185241699\n",
      "=======================================================================\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 4, flash: 3.4630037546157837\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 4, vanilla: 17.455777645111084\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 4, optim: 5.559507966041565\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 4, flash-ref: 1.7905337810516357\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 4, vanilla-ref: 4.612319350242615\n",
      "=======================================================================\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 8, flash: 3.5759488344192505\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 8, vanilla: 19.991883277893066\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 8, optim: 6.977637887001038\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 8, flash-ref: 1.7945010662078857\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 8, vanilla-ref: 4.727899193763733\n",
      "=======================================================================\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 16, flash: 3.580857038497925\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 16, vanilla: 19.1587735414505\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 16, optim: 9.36732280254364\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 16, flash-ref: 1.793500542640686\n",
      "bsz: 4, prefill: [65536 65536 65536 65536], input: 16, vanilla-ref: 4.944325804710388\n",
      "=======================================================================\n",
      "***********************************************************************\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 1, flash: 3.6688472032546997\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 1, vanilla: 33.01150417327881\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 1, optim: 9.21896493434906\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 1, flash-ref: 3.6645346879959106\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 1, vanilla-ref: 9.307384252548218\n",
      "=======================================================================\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 2, flash: 7.21043074131012\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 2, vanilla: 34.135294795036316\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 2, optim: 10.051493525505066\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 2, flash-ref: 3.657872796058655\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 2, vanilla-ref: 9.237118244171143\n",
      "=======================================================================\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 4, flash: 7.315257668495178\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 4, vanilla: 36.69091522693634\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 4, optim: 11.476417899131775\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 4, flash-ref: 3.6724350452423096\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 4, vanilla-ref: 9.369331002235413\n",
      "=======================================================================\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 8, flash: 7.436080098152161\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 8, vanilla: 40.80657732486725\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 8, optim: 13.59052288532257\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 8, flash-ref: 3.6696317195892334\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 8, vanilla-ref: 9.368884682655334\n",
      "=======================================================================\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 16, flash: 7.495926380157471\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 16, vanilla: 38.62033414840698\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 16, optim: 18.6979159116745\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 16, flash-ref: 3.6649585962295532\n",
      "bsz: 8, prefill: [65536 65536 65536 65536 65536 65536 65536 65536], input: 16, vanilla-ref: 10.161896347999573\n",
      "=======================================================================\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_with_kvcache\n",
    "from transformers.models.llama.modeling_llama import(\n",
    "    repeat_kv,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "def benchmark_mqa_attn(attn_method, query_states, key_states, value_states, num_key_value_groups, seq_len):\n",
    "    bsz, kv_len, kv_heads, head_dim = key_states.shape\n",
    "    if attn_method == 'flash':\n",
    "        # warm up\n",
    "        for i in range(100):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            flash_attn_with_kvcache(q=query_states, k_cache=key_states, v_cache=value_states, cache_seqlens=seq_len, softmax_scale=1/torch.sqrt(torch.tensor(head_dim, dtype=torch.float16)), causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        T = 2000\n",
    "        time1 = time.time()\n",
    "        for i in range(T):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            attn_output = flash_attn_with_kvcache(q=query_states, k_cache=key_states, v_cache=value_states, cache_seqlens=seq_len, softmax_scale=1/torch.sqrt(torch.tensor(head_dim, dtype=torch.float16)), causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        latency = (time.time()-time1) / T * 1000\n",
    "\n",
    "    elif attn_method == 'flash-ref':\n",
    "        # warm up\n",
    "        for i in range(100):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            flash_attn_with_kvcache(q=query_states[:,:,:4], k_cache=key_states, v_cache=value_states, cache_seqlens=seq_len, softmax_scale=1/torch.sqrt(torch.tensor(head_dim, dtype=torch.float16)), causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        T = 2000\n",
    "        time1 = time.time()\n",
    "        for i in range(T):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            attn_output = flash_attn_with_kvcache(q=query_states[:,:,:4], k_cache=key_states, v_cache=value_states, cache_seqlens=seq_len, softmax_scale=1/torch.sqrt(torch.tensor(head_dim, dtype=torch.float16)), causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        latency = (time.time()-time1) / T * 1000\n",
    "    \n",
    "    elif attn_method == 'vanilla':\n",
    "        bsz, kv_len, kv_heads, head_dim = key_states.shape\n",
    "        # warm up\n",
    "        for i in range(100):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            key_states_cp = repeat_kv(key_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            value_states_cp = repeat_kv(value_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            attn_weights = torch.matmul(query_states.transpose(1,2), key_states_cp.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states_cp.transpose(1,2))\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        torch.cuda.synchronize()\n",
    "        T = 2000\n",
    "        time1 = time.time()\n",
    "        for i in range(T):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            key_states_cp = repeat_kv(key_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            value_states_cp = repeat_kv(value_states.transpose(1,2), num_key_value_groups).transpose(1,2)\n",
    "            attn_weights = torch.matmul(query_states.transpose(1,2), key_states_cp.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states_cp.transpose(1,2))\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        torch.cuda.synchronize()\n",
    "        latency = (time.time()-time1) / T * 1000\n",
    "\n",
    "    elif attn_method == 'vanilla-ref':\n",
    "        bsz, kv_len, kv_heads, head_dim = key_states.shape\n",
    "        # warm up\n",
    "        for i in range(100):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            attn_weights = torch.matmul(query_states[:,:,:4].transpose(1,2), key_states.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states.transpose(1,2))\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        torch.cuda.synchronize()\n",
    "        T = 2000\n",
    "        time1 = time.time()\n",
    "        for i in range(T):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            attn_weights = torch.matmul(query_states[:,:,:4].transpose(1,2), key_states.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states.transpose(1,2))\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        torch.cuda.synchronize()\n",
    "        latency = (time.time()-time1) / T * 1000\n",
    "\n",
    "    elif attn_method == 'optim':\n",
    "        bsz, kv_len, kv_heads, head_dim = key_states.shape\n",
    "        _, query_len, _, _ = query_states.shape\n",
    "        for i in range(100):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            query_states_cp = query_states.transpose(1,2).reshape(bsz, kv_heads, num_key_value_groups*query_len, head_dim)\n",
    "            attn_weights = torch.matmul(query_states_cp, key_states.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim) # [bsz, 4, 8*seq, prefill+seq]\n",
    "            # [TODO] add attn mask here....\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states.transpose(1,2)) # [bsz, 4, 8*seq, 128]\n",
    "            attn_output = attn_output.reshape(bsz, kv_heads*num_key_value_groups, query_len, head_dim).transpose(1, 2).contiguous() # [bsz, seq, 32, 128]\n",
    "        torch.cuda.synchronize()\n",
    "        T = 2000\n",
    "        time1 = time.time()\n",
    "        for i in range(T):\n",
    "            query_states.normal_()\n",
    "            key_states.normal_()\n",
    "            value_states.normal_()\n",
    "            query_states_cp = query_states.transpose(1,2).reshape(bsz, kv_heads, num_key_value_groups*query_len, head_dim)\n",
    "            attn_weights = torch.matmul(query_states_cp, key_states.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim) # [bsz, 4, 8*seq, prefill+seq]\n",
    "            # [TODO] add attn mask here....\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states.transpose(1,2)) # [bsz, 4, 8*seq, 128]\n",
    "            attn_output = attn_output.reshape(bsz, kv_heads*num_key_value_groups, query_len, head_dim).transpose(1, 2).contiguous() # [bsz, seq, 32, 128]\n",
    "        torch.cuda.synchronize()\n",
    "        latency = (time.time()-time1) / T * 1000\n",
    "\n",
    "    return latency\n",
    "\n",
    "\n",
    "num_key_value_groups = 8\n",
    "prefill = 1024*64\n",
    "\n",
    "# bsz_list = [1,2,3,4,5,6,7,8]\n",
    "bsz_list = [1,2,4,8]\n",
    "input_list = [1,2,4,8,16]\n",
    "\n",
    "for bsz in bsz_list:\n",
    "    for input in input_list:\n",
    "        seq_len = torch.tensor([prefill]*bsz, dtype=torch.int32, device=\"cuda:1\")\n",
    "        query_states = torch.randn(bsz, input, 32, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "        key_states = torch.randn(bsz, prefill+input, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "        value_states = torch.randn(bsz, prefill+input, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "        latency = benchmark_mqa_attn('flash', query_states, key_states, value_states, num_key_value_groups, seq_len)\n",
    "        print(f\"bsz: {bsz}, prefill: {seq_len.cpu().numpy()}, input: {input}, flash: {latency}\")\n",
    "        latency = benchmark_mqa_attn('vanilla', query_states, key_states, value_states, num_key_value_groups, seq_len)\n",
    "        print(f\"bsz: {bsz}, prefill: {seq_len.cpu().numpy()}, input: {input}, vanilla: {latency}\")\n",
    "        latency = benchmark_mqa_attn('optim', query_states, key_states, value_states, num_key_value_groups, seq_len)\n",
    "        print(f\"bsz: {bsz}, prefill: {seq_len.cpu().numpy()}, input: {input}, optim: {latency}\")\n",
    "        latency = benchmark_mqa_attn('flash-ref', query_states, key_states, value_states, num_key_value_groups, seq_len)\n",
    "        print(f\"bsz: {bsz}, prefill: {seq_len.cpu().numpy()}, input: {input}, flash-ref: {latency}\")\n",
    "        latency = benchmark_mqa_attn('vanilla-ref', query_states, key_states, value_states, num_key_value_groups, seq_len)\n",
    "        print(f\"bsz: {bsz}, prefill: {seq_len.cpu().numpy()}, input: {input}, vanilla-ref: {latency}\")\n",
    "        print(\"=======================================================================\")\n",
    "    print(\"***********************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3586692810058594"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 2\n",
    "num_key_value_groups = 8\n",
    "prefill = 122880\n",
    "seq_len = torch.tensor([prefill]*bsz, dtype=torch.int32, device=\"cuda:1\")\n",
    "\n",
    "\n",
    "query_states = torch.randn(bsz, 1, 32, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "key_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "value_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "\n",
    "benchmark_mqa_attn('flash', query_states, key_states, value_states, num_key_value_groups, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5221462249755859"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 3\n",
    "num_key_value_groups = 8\n",
    "prefill = 122880\n",
    "seq_len = torch.tensor([prefill]*bsz, dtype=torch.int32, device=\"cuda:1\")\n",
    "\n",
    "\n",
    "query_states = torch.randn(bsz, 1, 32, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "key_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "value_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "\n",
    "benchmark_mqa_attn('flash', query_states, key_states, value_states, num_key_value_groups, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810362339019775"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 4\n",
    "num_key_value_groups = 8\n",
    "prefill = 122880\n",
    "seq_len = torch.tensor([prefill]*bsz, dtype=torch.int32, device=\"cuda:1\")\n",
    "\n",
    "\n",
    "query_states = torch.randn(bsz, 1, 32, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "key_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "value_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "\n",
    "benchmark_mqa_attn('flash', query_states, key_states, value_states, num_key_value_groups, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.401186466217041"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 8\n",
    "num_key_value_groups = 8\n",
    "prefill = 122880\n",
    "seq_len = torch.tensor([prefill]*bsz, dtype=torch.int32, device=\"cuda:1\")\n",
    "\n",
    "query_states = torch.randn(bsz, 1, 32, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "key_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "value_states = torch.randn(bsz, prefill+1, 32//num_key_value_groups, 128, dtype=torch.float16, device=\"cuda:1\")\n",
    "\n",
    "benchmark_mqa_attn('flash', query_states, key_states, value_states, num_key_value_groups, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 122881, 4, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
