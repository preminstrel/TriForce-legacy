{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a720b6f38f94599b5635781458b3d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/hanshis/workspace/LongContextInfer')\n",
    "\n",
    "import torch\n",
    "from models.modeling_llama import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model = \"NousResearch/Yarn-Llama-2-7b-128k\"\n",
    "model = \"01-ai/Yi-6B-200K\"\n",
    "\n",
    "target = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16, device_map=\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.model.layers[0].self_attn.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " \n",
      " I  have  a  dog . \n",
      " "
     ]
    }
   ],
   "source": [
    "from models.cache_utils import FlashSimpleCache\n",
    "\n",
    "with torch.inference_mode():\n",
    "    cache = FlashSimpleCache(target, 1024)\n",
    "    prompts = \"Hello, my dog is cute\"\n",
    "    next_token = tokenizer.encode(prompts, return_tensors=\"pt\").to(target.device)\n",
    "    for i in range(100):\n",
    "        if next_token.shape == torch.Size([1]):\n",
    "            next_token = next_token.unsqueeze(0)\n",
    "        if next_token.shape == torch.Size([]):\n",
    "            next_token = next_token.unsqueeze(0).unsqueeze(0)\n",
    "        logits = target(input_ids=next_token, kv_cache=cache, graph_cache=None).logits\n",
    "        next_token = torch.argmax(logits[:, -1, :])\n",
    "        print(tokenizer.decode(next_token), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e25677a836b4cdf844a90ff6e9ba7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys, torch\n",
    "sys.path.append('/home/hanshis/workspace/LongContextInfer')\n",
    "\n",
    "from models.modeling_batch_llama import LlamaForCausalLM\n",
    "from models.batch_cache import BatchSimpleCache\n",
    "from transformers import AutoTokenizer\n",
    "model = \"01-ai/Yi-6B-200K\"\n",
    "target_bsz = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16, device_map=\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    prompts = \"Hello, my dog is cute\"\n",
    "    next_token = tokenizer.encode(prompts, return_tensors=\"pt\").to(target_bsz.device)\n",
    "    cache_bsz = BatchSimpleCache(target_bsz, 1024, 1)\n",
    "    n=0\n",
    "    cache_bsz.reset()\n",
    "    gen_tokens = torch.zeros((1, 100), dtype=torch.long, device=target_bsz.device)\n",
    "    while n < 100:\n",
    "        logits = target_bsz(input_ids=next_token, kv_cache=cache_bsz, graph_cache=None).logits\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        gen_tokens[:, n] = next_token.squeeze()\n",
    "        next_token = next_token.unsqueeze(1)\n",
    "        n += 1\n",
    "    tokenizer.batch_decode(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n\\nI have a dog.\\n']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(gen_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(target.model.layers[1].self_attn.rotary_emb.cos_cached, target_bsz.model.layers[1].self_attn.rotary_emb.cos_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[0,:,:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(cache.key_cache[0,:,:1], cache_bsz.key_cache[0,:,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(cache.key_cache[1,:,:1], cache_bsz.key_cache[1,:,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_models_identical(model1, model2):\n",
    "    # 检查模型参数\n",
    "    params1 = list(model1.parameters())\n",
    "    params2 = list(model2.parameters())\n",
    "    if len(params1) != len(params2):\n",
    "        print(\"false\")\n",
    "        return False\n",
    "    for p1, p2 in zip(params1, params2):\n",
    "        if p1.data.ne(p2.data).sum() > 0:  # 检查是否有不同的元素\n",
    "            print(\"false\")\n",
    "            return False\n",
    "    \n",
    "    # 检查模型的缓冲区\n",
    "    buffers1 = list(model1.buffers())\n",
    "    buffers2 = list(model2.buffers())\n",
    "    if len(buffers1) != len(buffers2):\n",
    "        print(\"false\")\n",
    "        return False\n",
    "    for b1, b2 in zip(buffers1, buffers2):\n",
    "        if b1.data.ne(b2.data).sum() > 0:  # 检查是否有不同的元素\n",
    "            print(\"false\")\n",
    "            return False\n",
    "    \n",
    "    # 如果所有检查都通过了，则模型是完全相同的\n",
    "    return True\n",
    "\n",
    "check_models_identical(target, target_bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 2 --prefill 491520\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 4 --prefill 245760\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 6 --prefill 163840\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 8 --prefill 122880\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 10 --prefill 98304\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 12 --prefill 81920\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 14 --prefill 70217\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 16 --prefill 61440\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 18 --prefill 54613\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 20 --prefill 49152\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 22 --prefill 44683\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 24 --prefill 40960\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 26 --prefill 37809\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 28 --prefill 35108\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 30 --prefill 32768\n",
      "CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz 32 --prefill 30720\n"
     ]
    }
   ],
   "source": [
    "for i in [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]:\n",
    "    print(f\"CUDA_VISIBLE_DEVICES=0 python benchmark/batch_mqa.py --bsz {i} --prefill {122880*8//i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
