>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 128, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 128, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 128, 4102, 4, 128])
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:11,  1.95it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:03,  5.51it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:05,  3.88it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:01, 10.03it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:21,  1.01it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 57.76it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:24,  1.12s/it]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 20.56it/s]
Resolving data files:  26%|██▌       | 6/23 [00:01<00:02,  7.63it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:20,  1.07it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:26,  1.22s/it]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 21.18it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:26,  1.22s/it]Resolving data files:  17%|█▋        | 4/23 [00:01<00:05,  3.75it/s]Resolving data files:  17%|█▋        | 4/23 [00:01<00:04,  4.29it/s]Resolving data files:   9%|▊         | 2/23 [00:01<00:12,  1.70it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:26,  1.20s/it]Resolving data files:   9%|▊         | 2/23 [00:01<00:13,  1.55it/s]Resolving data files:  26%|██▌       | 6/23 [00:01<00:03,  5.17it/s]Resolving data files:  17%|█▋        | 4/23 [00:01<00:05,  3.64it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 14.43it/s]
Resolving data files:  13%|█▎        | 3/23 [00:01<00:08,  2.24it/s]Resolving data files:  30%|███       | 7/23 [00:01<00:02,  6.69it/s]Resolving data files:  87%|████████▋ | 20/23 [00:01<00:00, 19.56it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.45it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 13.66it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 14.83it/s]
Resolving data files:  39%|███▉      | 9/23 [00:01<00:01,  7.54it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 13.12it/s]
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 128, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 128, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 128, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.03150354325771332 ms
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 64, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 64, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 64, 4102, 4, 128])
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s][Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 64, 4102, 4, 128])
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.98it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.62it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:13,  1.58it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:05,  3.89it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:07,  2.86it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:18,  1.19it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:13,  1.62it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:05,  3.68it/s]Resolving data files:  61%|██████    | 14/23 [00:00<00:00, 19.52it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 24.10it/s]
Resolving data files:   4%|▍         | 1/23 [00:00<00:21,  1.04it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:17,  1.23it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:25,  1.16s/it]Resolving data files:  74%|███████▍  | 17/23 [00:01<00:00, 24.59it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 19.52it/s]
Resolving data files:  26%|██▌       | 6/23 [00:01<00:02,  6.97it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:08,  2.41it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:21,  1.00it/s]Resolving data files:  43%|████▎     | 10/23 [00:01<00:01, 11.71it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:05,  3.66it/s]Resolving data files:  35%|███▍      | 8/23 [00:01<00:01,  7.77it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.14it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:10,  1.95it/s]Resolving data files:  57%|█████▋    | 13/23 [00:01<00:00, 14.52it/s]Resolving data files:  39%|███▉      | 9/23 [00:01<00:00, 14.23it/s]Resolving data files:  13%|█▎        | 3/23 [00:01<00:06,  2.97it/s]Resolving data files:  61%|██████    | 14/23 [00:01<00:00, 19.63it/s]Resolving data files:   9%|▊         | 2/23 [00:01<00:12,  1.68it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 18.26it/s]
Resolving data files:  39%|███▉      | 9/23 [00:01<00:01,  9.88it/s]Resolving data files:  70%|██████▉   | 16/23 [00:01<00:00, 13.12it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 14.35it/s]
Resolving data files:  13%|█▎        | 3/23 [00:01<00:08,  2.30it/s]Resolving data files:  65%|██████▌   | 15/23 [00:01<00:00, 15.85it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 14.95it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.19it/s]
Resolving data files:  17%|█▋        | 4/23 [00:01<00:05,  3.48it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.73it/s]
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 64, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 64, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.02945665270090103 ms
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 32, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 32, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:17,  1.28it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:10,  2.03it/s]Resolving data files:  26%|██▌       | 6/23 [00:00<00:02,  8.12it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:18,  1.18it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:19,  1.11it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:10,  2.00it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 23.84it/s]
Resolving data files:   9%|▊         | 2/23 [00:00<00:09,  2.33it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:21,  1.00it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:07,  2.99it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 21.61it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:24,  1.11s/it]Resolving data files:  17%|█▋        | 4/23 [00:00<00:03,  6.02it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:25,  1.18s/it]Resolving data files:  26%|██▌       | 6/23 [00:01<00:02,  6.00it/s]Resolving data files:  22%|██▏       | 5/23 [00:00<00:02,  6.25it/s]Resolving data files:  83%|████████▎ | 19/23 [00:01<00:00, 18.13it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.22it/s]
Resolving data files:  26%|██▌       | 6/23 [00:01<00:03,  5.65it/s]Resolving data files:  52%|█████▏    | 12/23 [00:01<00:00, 12.80it/s]Resolving data files:  57%|█████▋    | 13/23 [00:01<00:00, 21.50it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.36it/s]
Resolving data files:  87%|████████▋ | 20/23 [00:01<00:00, 21.50it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.63it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:12,  1.68it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 19.79it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:16,  1.30it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 14.60it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 19.33it/s]
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 32, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 32, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.02863672375679016 ms
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 16, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 16, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:18,  1.19it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:18,  1.17it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 26.65it/s]
Resolving data files:   4%|▍         | 1/23 [00:00<00:19,  1.13it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:09,  2.33it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:05,  3.75it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 21.97it/s]
Resolving data files:  13%|█▎        | 3/23 [00:01<00:06,  3.16it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.46it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:08,  2.54it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:25,  1.16s/it]Resolving data files:   4%|▍         | 1/23 [00:01<00:26,  1.19s/it]Resolving data files:   9%|▊         | 2/23 [00:00<00:05,  3.57it/s]Resolving data files:  17%|█▋        | 4/23 [00:01<00:04,  3.94it/s]Resolving data files:  35%|███▍      | 8/23 [00:01<00:01,  7.90it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.27it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:09,  2.13it/s]Resolving data files:  22%|██▏       | 5/23 [00:00<00:02,  8.16it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:26,  1.22s/it]Resolving data files:  48%|████▊     | 11/23 [00:01<00:01,  9.90it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 16.01it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 18.55it/s]
Resolving data files:  39%|███▉      | 9/23 [00:01<00:01, 11.63it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 19.79it/s]
Resolving data files:  96%|█████████▌| 22/23 [00:01<00:00, 22.18it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 14.75it/s]
Resolving data files:  26%|██▌       | 6/23 [00:00<00:02,  7.14it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 23.81it/s]
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 16, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 16, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.027781441807746887 ms
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 8, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 8, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:16,  1.31it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:19,  1.14it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.43it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:04,  4.01it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:07,  2.80it/s]Resolving data files:  22%|██▏       | 5/23 [00:01<00:02,  6.02it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:16,  1.36it/s]Resolving data files:   9%|▊         | 2/23 [00:01<00:10,  1.96it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 19.71it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:23,  1.07s/it]Resolving data files:   9%|▊         | 2/23 [00:01<00:10,  2.06it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:17,  1.27it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:28,  1.31s/it]Resolving data files:  13%|█▎        | 3/23 [00:01<00:06,  3.04it/s]Resolving data files:  52%|█████▏    | 12/23 [00:00<00:00, 17.58it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 25.32it/s]
Resolving data files:  22%|██▏       | 5/23 [00:01<00:03,  4.88it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:09,  2.30it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 26.87it/s]
Resolving data files:  30%|███       | 7/23 [00:01<00:02,  6.79it/s]Resolving data files:  61%|██████    | 14/23 [00:01<00:00, 18.98it/s]Resolving data files:  91%|█████████▏| 21/23 [00:01<00:00, 23.64it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 16.34it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:17,  1.18it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 13.83it/s]
Resolving data files:  57%|█████▋    | 13/23 [00:02<00:01,  7.76it/s]Resolving data files: 100%|██████████| 23/23 [00:02<00:00, 11.23it/s]
Resolving data files:  83%|████████▎ | 19/23 [00:02<00:00, 13.08it/s]Resolving data files: 100%|██████████| 23/23 [00:02<00:00, 11.08it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.14it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 12.05it/s]
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 8, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 8, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.02755613625049591 ms
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 4, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 4, 224, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 4, 4102, 4, 128])
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:12,  1.78it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:06,  3.26it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:12,  1.70it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:05,  3.38it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:19,  1.13it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:14,  1.55it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:19,  1.13it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:08,  2.37it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 22.04it/s]
Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.42it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 23.30it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:09,  2.21it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:08,  2.62it/s]Resolving data files:   9%|▊         | 2/23 [00:01<00:10,  2.10it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:08,  2.52it/s]Resolving data files:  13%|█▎        | 3/23 [00:01<00:05,  3.37it/s]Resolving data files:  22%|██▏       | 5/23 [00:00<00:02,  7.54it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 23.04it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:29,  1.34s/it]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.13it/s]
Resolving data files:  57%|█████▋    | 13/23 [00:01<00:00, 19.92it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 18.11it/s]
Resolving data files:  22%|██▏       | 5/23 [00:01<00:03,  5.75it/s]Resolving data files:  17%|█▋        | 4/23 [00:01<00:03,  4.80it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.93it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:23,  1.05s/it]Resolving data files:  43%|████▎     | 10/23 [00:01<00:00, 14.23it/s]Resolving data files:   9%|▊         | 2/23 [00:01<00:10,  1.95it/s]Resolving data files:  70%|██████▉   | 16/23 [00:01<00:00, 21.38it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.87it/s]
Resolving data files:  96%|█████████▌| 22/23 [00:01<00:00, 28.67it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 16.55it/s]
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 4, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 4, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 4, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.028506942093372345 ms
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 2, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 2, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.00it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.62it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.42it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:14,  1.47it/s]Resolving data files:  22%|██▏       | 5/23 [00:00<00:02,  7.64it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:04,  5.23it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.40it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:08,  2.62it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:05,  4.32it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.45it/s]Resolving data files:  35%|███▍      | 8/23 [00:00<00:01, 12.62it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:04,  4.16it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:03,  5.52it/s]Resolving data files:  70%|██████▉   | 16/23 [00:01<00:00, 20.30it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 20.91it/s]
Resolving data files:  52%|█████▏    | 12/23 [00:01<00:00, 17.96it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:05,  4.19it/s]Resolving data files:  13%|█▎        | 3/23 [00:00<00:05,  3.93it/s]Resolving data files:  17%|█▋        | 4/23 [00:00<00:02,  8.27it/s]Resolving data files:  39%|███▉      | 9/23 [00:01<00:01, 12.01it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 21.55it/s]
Resolving data files:  91%|█████████▏| 21/23 [00:01<00:00, 32.14it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 19.20it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:22,  1.03s/it]Resolving data files:  13%|█▎        | 3/23 [00:01<00:06,  3.21it/s]Resolving data files:  26%|██▌       | 6/23 [00:00<00:01,  9.77it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 30.38it/s]
Resolving data files:  26%|██▌       | 6/23 [00:01<00:02,  6.81it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 20.32it/s]
Resolving data files:  70%|██████▉   | 16/23 [00:01<00:00, 21.60it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 18.37it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:30,  1.41s/it]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 16.35it/s]
Resolving data files:  17%|█▋        | 4/23 [00:00<00:04,  4.73it/s]Resolving data files:  52%|█████▏    | 12/23 [00:00<00:00, 17.57it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 24.15it/s]
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 2, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 2, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 2, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.027390219271183014 ms
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 1, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 1, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.40it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:17,  1.26it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:08,  2.56it/s]Resolving data files:  30%|███       | 7/23 [00:00<00:01,  9.99it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:13,  1.60it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 23.42it/s]
Resolving data files:   4%|▍         | 1/23 [00:00<00:20,  1.06it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:16,  1.30it/s]Resolving data files:  26%|██▌       | 6/23 [00:01<00:02,  7.86it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 21.41it/s]
Resolving data files:   9%|▊         | 2/23 [00:01<00:09,  2.19it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:23,  1.09s/it]Resolving data files:   4%|▍         | 1/23 [00:01<00:25,  1.18s/it]Resolving data files:  22%|██▏       | 5/23 [00:01<00:02,  6.16it/s]Resolving data files:  17%|█▋        | 4/23 [00:00<00:03,  4.91it/s]Resolving data files:  17%|█▋        | 4/23 [00:01<00:03,  4.78it/s]Resolving data files:  17%|█▋        | 4/23 [00:01<00:04,  3.84it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 22.63it/s]
Resolving data files:  17%|█▋        | 4/23 [00:01<00:04,  3.81it/s]Resolving data files:   4%|▍         | 1/23 [00:01<00:30,  1.36s/it]Resolving data files:  35%|███▍      | 8/23 [00:01<00:01,  9.83it/s]Resolving data files:  83%|████████▎ | 19/23 [00:01<00:00, 24.54it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 16.30it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 26.57it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 16.10it/s]
Resolving data files:  48%|████▊     | 11/23 [00:01<00:01, 11.71it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.63it/s]
Resolving data files:  91%|█████████▏| 21/23 [00:01<00:00, 18.94it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 15.31it/s]
Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 18.82it/s]
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 1, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 1, 224, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 1, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.028180032968521118 ms
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/4 on cuda:0, shape: torch.Size([32, 128, 224, 8, 128])
[Distributed Cache] Initiated for 2/4 on cuda:2, shape: torch.Size([32, 128, 224, 8, 128])
[Distributed Cache] Initiated for 1/4 on cuda:1, shape: torch.Size([32, 128, 224, 8, 128])
[Distributed Cache] Initiated for 3/4 on cuda:3, shape: torch.Size([32, 128, 224, 8, 128])
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/home/hanshis/workspace/LongContextInfer/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/home/hanshis/workspace/LongContextInfer/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.05 GiB. GPU 0 has a total capacity of 44.32 GiB of which 8.34 GiB is free. Including non-PyTorch memory, this process has 35.96 GiB memory in use. Of the allocated memory 35.55 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/home/hanshis/workspace/LongContextInfer/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/home/hanshis/workspace/LongContextInfer/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.05 GiB. GPU 2 has a total capacity of 44.32 GiB of which 8.34 GiB is free. Including non-PyTorch memory, this process has 35.96 GiB memory in use. Of the allocated memory 35.55 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/home/hanshis/workspace/LongContextInfer/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/home/hanshis/workspace/LongContextInfer/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.05 GiB. GPU 1 has a total capacity of 44.32 GiB of which 8.34 GiB is free. Including non-PyTorch memory, this process has 35.96 GiB memory in use. Of the allocated memory 35.55 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/home/hanshis/workspace/LongContextInfer/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/home/hanshis/workspace/LongContextInfer/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.05 GiB. GPU 3 has a total capacity of 44.32 GiB of which 8.34 GiB is free. Including non-PyTorch memory, this process has 35.96 GiB memory in use. Of the allocated memory 35.55 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-03-22 18:37:03,970] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1511905 closing signal SIGTERM
[2024-03-22 18:37:03,970] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1511906 closing signal SIGTERM
[2024-03-22 18:37:03,971] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1511907 closing signal SIGTERM
[2024-03-22 18:37:04,398] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1511904) of binary: /home/hanshis/anaconda3/envs/torch/bin/python
Traceback (most recent call last):
  File "/home/hanshis/anaconda3/envs/torch/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
test/TP_baseline.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-22_18:37:03
  host      : lovelace.ece.local.cmu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1511904)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 1/4 on cuda:1, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Cache] Initiated for 3/4 on cuda:3, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Cache] Initiated for 0/4 on cuda:0, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Cache] Initiated for 2/4 on cuda:2, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Retrieval Cache] Initiated for 1/4 on cuda:1, shape: torch.Size([32, 64, 4102, 8, 128])
[Distributed Retrieval Cache] Initiated for 2/4 on cuda:2, shape: torch.Size([32, 64, 4102, 8, 128])
[Distributed Retrieval Cache] Initiated for 3/4 on cuda:3, shape: torch.Size([32, 64, 4102, 8, 128])
[Distributed Retrieval Cache] Initiated for 0/4 on cuda:0, shape: torch.Size([32, 64, 4102, 8, 128])
Rank 1/4 (Device cuda:0) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]
Rank 2/4 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]
Rank 3/4 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]
Rank 4/4 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:11,  1.98it/s]Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:   9%|▊         | 2/23 [00:00<00:07,  2.83it/s]Resolving data files:  61%|██████    | 14/23 [00:00<00:00, 21.14it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 24.06it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:22,  1.01s/it]Resolving data files:  91%|█████████▏| 21/23 [00:01<00:00, 22.29it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 18.65it/s]
Resolving data files:   4%|▍         | 1/23 [00:01<00:23,  1.08s/it]Resolving data files:  57%|█████▋    | 13/23 [00:01<00:00, 14.04it/s]Resolving data files:   4%|▍         | 1/23 [00:00<00:15,  1.38it/s]Resolving data files: 100%|██████████| 23/23 [00:01<00:00, 17.36it/s]
Resolving data files:  57%|█████▋    | 13/23 [00:00<00:00, 20.36it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 27.33it/s]
[Distributed Cache] Reset for 1/4 on cuda:1, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Retrieval Cache] Reset for 1/4 on cuda:1, shape: torch.Size([32, 64, 4102, 8, 128])
[Distributed Cache] Reset for 2/4 on cuda:2, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Retrieval Cache] Reset for 2/4 on cuda:2, shape: torch.Size([32, 64, 4102, 8, 128])
[Distributed Cache] Reset for 3/4 on cuda:3, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Retrieval Cache] Reset for 3/4 on cuda:3, shape: torch.Size([32, 64, 4102, 8, 128])
[Distributed Cache] Reset for 0/4 on cuda:0, shape: torch.Size([32, 64, 224, 8, 128])
[Distributed Retrieval Cache] Reset for 0/4 on cuda:0, shape: torch.Size([32, 64, 4102, 8, 128])
[rank3]:[E ProcessGroupNCCL.cpp:1182] [Rank 3] NCCL watchdog thread terminated with exception: CUDA error: unknown error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fdd764f4d87 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7fdd764a575f in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7fdd765c58a8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x6c (0x7fdd2b8983ac in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7fdd2b89c4c8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x15a (0x7fdd2b89fbfa in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7fdd2b8a0839 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xdbbf4 (0x7fdd75adbbf4 in /home/hanshis/anaconda3/envs/torch/bin/../lib/libstdc++.so.6)
frame #8: <unknown function> + 0x9f802 (0x7fdd76e9f802 in /lib64/libc.so.6)
frame #9: <unknown function> + 0x3f450 (0x7fdd76e3f450 in /lib64/libc.so.6)

[rank0]:[E ProcessGroupNCCL.cpp:1182] [Rank 0] NCCL watchdog thread terminated with exception: CUDA error: unknown error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f983b4ced87 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f983b47f75f in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f983b59f8a8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x6c (0x7f97f08983ac in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7f97f089c4c8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x15a (0x7f97f089fbfa in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f97f08a0839 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xdbbf4 (0x7f983aadbbf4 in /home/hanshis/anaconda3/envs/torch/bin/../lib/libstdc++.so.6)
frame #8: <unknown function> + 0x9f802 (0x7f983be9f802 in /lib64/libc.so.6)
frame #9: <unknown function> + 0x3f450 (0x7f983be3f450 in /lib64/libc.so.6)

Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 65, in <module>
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 65, in <module>
[rank1]:[E ProcessGroupNCCL.cpp:1182] [Rank 1] NCCL watchdog thread terminated with exception: CUDA error: unknown error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f4fa3781d87 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f4fa373275f in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f4fa3b308a8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x6c (0x7f4f58a983ac in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7f4f58a9c4c8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x15a (0x7f4f58a9fbfa in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f4f58aa0839 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xdbbf4 (0x7f4fa2cdbbf4 in /home/hanshis/anaconda3/envs/torch/bin/../lib/libstdc++.so.6)
frame #8: <unknown function> + 0x9f802 (0x7f4fa429f802 in /lib64/libc.so.6)
frame #9: <unknown function> + 0x3f450 (0x7f4fa423f450 in /lib64/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:1182] [Rank 2] NCCL watchdog thread terminated with exception: CUDA error: unknown error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f1ed1b81d87 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f1ed1b3275f in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f1ed1f9d8a8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x6c (0x7f1e86e983ac in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7f1e86e9c4c8 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x15a (0x7f1e86e9fbfa in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x7f1e86ea0839 in /home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xdbbf4 (0x7f1ed10dbbf4 in /home/hanshis/anaconda3/envs/torch/bin/../lib/libstdc++.so.6)
frame #8: <unknown function> + 0x9f802 (0x7f1ed269f802 in /lib64/libc.so.6)
frame #9: <unknown function> + 0x3f450 (0x7f1ed263f450 in /lib64/libc.so.6)

Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 65, in <module>
    baseline_latency, gen_tokens = Baseline_Dist(tokenizer, llm, input_ids, max_len=gen_len, temperature=temperature, top_p=top_p, local_rank=local_rank)Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/test/TP_baseline.py", line 65, in <module>
