>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 128, 512, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 128, 512, 4, 128])
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 0 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 2 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 5 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 4 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 7 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 3 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 1 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/hanshi/TriForce/test/TP_baseline.py", line 47, in <module>
    llm = DistributedLlama(model_name_or_path=model_name_or_path, local_rank=local_rank, world_size=world_size, prefill=prefill, bsz=bsz, gen_len=gen_len, temperature=temperature, top_p=top_p, flash_attn=True)
  File "/root/hanshi/TriForce/models/TP_llama.py", line 63, in __init__
    self.retrieval_cache = DistributedBatchRetrievalCache(self.config, max_budget=retrieval_budget, bsz=bsz, device=self.device, prefill=prefill, chunk_size=retrieval_chunk_size, gamma=gamma)
  File "/root/hanshi/TriForce/models/batch_cache.py", line 424, in __init__
    self.value_cache=torch.zeros([self.layers, bsz, self.real_budget, self.num_heads, self.head_dim], dtype=dtype).to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.02 GiB. GPU 6 has a total capacity of 23.69 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 21.11 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-03-23 08:07:19,833] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 970726 closing signal SIGTERM
[2024-03-23 08:07:19,833] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 970728 closing signal SIGTERM
[2024-03-23 08:07:19,835] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 970729 closing signal SIGTERM
[2024-03-23 08:07:19,837] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 970731 closing signal SIGTERM
[2024-03-23 08:07:19,837] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 970732 closing signal SIGTERM
[2024-03-23 08:07:20,431] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 970725) of binary: /root/software/miniconda3/envs/triforce/bin/python
Traceback (most recent call last):
  File "/root/software/miniconda3/envs/triforce/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.1+cu118', 'console_scripts', 'torchrun')())
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
test/TP_baseline.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-23_08:07:19
  host      : rslserver-2
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 970727)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-23_08:07:19
  host      : rslserver-2
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 970730)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-23_08:07:19
  host      : rslserver-2
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 970725)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 64, 512, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 64, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 64, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 64, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
[2024-03-23 08:07:41,713] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971976 closing signal SIGTERM
[2024-03-23 08:07:41,714] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971977 closing signal SIGTERM
[2024-03-23 08:07:41,715] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971978 closing signal SIGTERM
[2024-03-23 08:07:41,718] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971979 closing signal SIGTERM
[2024-03-23 08:07:41,720] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971980 closing signal SIGTERM
[2024-03-23 08:07:41,721] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971981 closing signal SIGTERM
[2024-03-23 08:07:41,723] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 971982 closing signal SIGTERM
[2024-03-23 08:07:42,534] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -15) local_rank: 0 (pid: 971975) of binary: /root/software/miniconda3/envs/triforce/bin/python
Traceback (most recent call last):
  File "/root/software/miniconda3/envs/triforce/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.1+cu118', 'console_scripts', 'torchrun')())
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
test/TP_baseline.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-23_08:07:41
  host      : rslserver-2
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 971975)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 971975
========================================================
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 32, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 32, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 32, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 32, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 32, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.020134106278419495 ms
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed>>>> Flash Attention installed

>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 16, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 16, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.51it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 16, 4102, 4, 128])
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 16, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 16, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.019873782992362976 ms
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash Attention installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
>>>> Flash RoPE installed
[Distributed Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Initiated for 3/8 on cuda:3, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 4/8 on cuda:4, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 2/8 on cuda:2, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 1/8 on cuda:1, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 6/8 on cuda:6, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 5/8 on cuda:5, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Retrieval Cache] Initiated for 0/8 on cuda:0, shape: torch.Size([32, 8, 4102, 4, 128])
Rank 1/8 (Device cuda:0) is initializing parameters
[Distributed Retrieval Cache] Initiated for 7/8 on cuda:7, shape: torch.Size([32, 8, 4102, 4, 128])
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]
Rank 2/8 (Device cuda:1) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
Rank 3/8 (Device cuda:2) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Rank 4/8 (Device cuda:3) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]
Rank 5/8 (Device cuda:4) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]
Rank 6/8 (Device cuda:5) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Rank 7/8 (Device cuda:6) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]
Rank 8/8 (Device cuda:7) is initializing parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
[Distributed Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 2/8 on cuda:2, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 1/8 on cuda:1, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 3/8 on cuda:3, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 4/8 on cuda:4, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 7/8 on cuda:7, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 5/8 on cuda:5, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 6/8 on cuda:6, shape: torch.Size([32, 8, 4102, 4, 128])
[Distributed Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 8, 512, 4, 128])
[Distributed Retrieval Cache] Reset for 0/8 on cuda:0, shape: torch.Size([32, 8, 4102, 4, 128])
[Baseline-Autoregressive] average latency: 0.01961747370660305 ms
[2024-03-23 08:11:38,352] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
Traceback (most recent call last):
  File "/root/software/miniconda3/envs/triforce/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.1+cu118', 'console_scripts', 'torchrun')())
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    result = agent.run()
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 727, in run
    result = self._invoke_run(role)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 868, in _invoke_run
    time.sleep(monitor_interval)
  File "/root/software/miniconda3/envs/triforce/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 984596 got signal: 15
