>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :128000, Decode Length :1, inference time:0.0014448713064193726s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :129024, Decode Length :1, inference time:0.001456095814704895s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :130048, Decode Length :1, inference time:0.0014631506204605103s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :131072, Decode Length :1, inference time:0.0014723680019378663s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :132096, Decode Length :1, inference time:0.0014814988374710083s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :133120, Decode Length :1, inference time:0.001493155837059021s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :134144, Decode Length :1, inference time:0.0014976842403411866s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :135168, Decode Length :1, inference time:0.0015065065622329712s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :136192, Decode Length :1, inference time:0.0015189676284790038s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :137216, Decode Length :1, inference time:0.0015291610956192016s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :138240, Decode Length :1, inference time:0.0015382912158966064s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :139264, Decode Length :1, inference time:0.0015473701953887939s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :140288, Decode Length :1, inference time:0.0015541844367980956s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :141312, Decode Length :1, inference time:0.001568690538406372s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :142336, Decode Length :1, inference time:0.0015769171714782714s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :143360, Decode Length :1, inference time:0.0015808519124984742s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :144384, Decode Length :1, inference time:0.0015901707410812377s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :145408, Decode Length :1, inference time:0.0016004047393798828s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :146432, Decode Length :1, inference time:0.001611427664756775s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :147456, Decode Length :1, inference time:0.0016206640005111694s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :148480, Decode Length :1, inference time:0.0016273226737976074s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :149504, Decode Length :1, inference time:0.0016379352807998656s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :150528, Decode Length :1, inference time:0.001647794485092163s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :151552, Decode Length :1, inference time:0.0016584306955337524s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :152576, Decode Length :1, inference time:0.0016668983697891236s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :153600, Decode Length :1, inference time:0.0016803393363952637s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :154624, Decode Length :1, inference time:0.001688314914703369s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :155648, Decode Length :1, inference time:0.0016949691772460938s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :156672, Decode Length :1, inference time:0.0017053495645523071s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :157696, Decode Length :1, inference time:0.0017130736112594604s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :158720, Decode Length :1, inference time:0.0017244277000427247s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :159744, Decode Length :1, inference time:0.0017326310873031615s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :160768, Decode Length :1, inference time:0.0017409226894378662s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :161792, Decode Length :1, inference time:0.001748649001121521s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :162816, Decode Length :1, inference time:0.0017559583187103272s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :163840, Decode Length :1, inference time:0.001767932653427124s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :164864, Decode Length :1, inference time:0.0017767143249511719s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :165888, Decode Length :1, inference time:0.0017844866514205933s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :166912, Decode Length :1, inference time:0.0018014504909515381s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :167936, Decode Length :1, inference time:0.0018075116872787475s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :168960, Decode Length :1, inference time:0.001817940592765808s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :169984, Decode Length :1, inference time:0.0018263561725616454s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :171008, Decode Length :1, inference time:0.0018350110054016113s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :172032, Decode Length :1, inference time:0.0018461413383483887s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :173056, Decode Length :1, inference time:0.0018556182384490967s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :174080, Decode Length :1, inference time:0.0018639644384384154s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :175104, Decode Length :1, inference time:0.0018740671873092652s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :176128, Decode Length :1, inference time:0.0018811295032501222s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :177152, Decode Length :1, inference time:0.001893064022064209s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :178176, Decode Length :1, inference time:0.0018989195823669433s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :179200, Decode Length :1, inference time:0.001908691167831421s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :180224, Decode Length :1, inference time:0.0019166749715805054s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :181248, Decode Length :1, inference time:0.0019261517524719238s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :182272, Decode Length :1, inference time:0.0019404239654541016s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :183296, Decode Length :1, inference time:0.0019482966661453246s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :184320, Decode Length :1, inference time:0.0019589394330978395s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :185344, Decode Length :1, inference time:0.0019670345783233643s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :186368, Decode Length :1, inference time:0.0019751533269882202s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :187392, Decode Length :1, inference time:0.0019894776344299317s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :188416, Decode Length :1, inference time:0.0019884003400802612s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :189440, Decode Length :1, inference time:0.002000564455986023s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :190464, Decode Length :1, inference time:0.00201527738571167s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :191488, Decode Length :1, inference time:0.002021117448806763s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :192512, Decode Length :1, inference time:0.0020347083806991576s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :193536, Decode Length :1, inference time:0.002035635232925415s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :194560, Decode Length :1, inference time:0.0020461748838424684s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :195584, Decode Length :1, inference time:0.0020569620132446288s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :196608, Decode Length :1, inference time:0.0020662858486175536s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :197632, Decode Length :1, inference time:0.0020750550031661988s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :198656, Decode Length :1, inference time:0.00208393919467926s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :199680, Decode Length :1, inference time:0.0020890439748764036s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :200704, Decode Length :1, inference time:0.002107343316078186s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :201728, Decode Length :1, inference time:0.0021141349077224733s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :202752, Decode Length :1, inference time:0.0021295980215072633s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :203776, Decode Length :1, inference time:0.0021360350847244264s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :204800, Decode Length :1, inference time:0.002144213080406189s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :205824, Decode Length :1, inference time:0.0021538418531417847s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :206848, Decode Length :1, inference time:0.002159817099571228s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :207872, Decode Length :1, inference time:0.002167301893234253s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :208896, Decode Length :1, inference time:0.002178512454032898s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :209920, Decode Length :1, inference time:0.0021858824491500853s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :210944, Decode Length :1, inference time:0.002192214369773865s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :211968, Decode Length :1, inference time:0.0022060114145278933s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :212992, Decode Length :1, inference time:0.002219135522842407s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :214016, Decode Length :1, inference time:0.0022259947061538696s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :215040, Decode Length :1, inference time:0.002233833909034729s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :216064, Decode Length :1, inference time:0.0022396870851516724s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :217088, Decode Length :1, inference time:0.002256577253341675s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :218112, Decode Length :1, inference time:0.0022667536735534667s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :219136, Decode Length :1, inference time:0.002275052547454834s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :220160, Decode Length :1, inference time:0.0022862123250961303s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :221184, Decode Length :1, inference time:0.0022956042289733886s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :222208, Decode Length :1, inference time:0.0023016784191131593s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :223232, Decode Length :1, inference time:0.002310094356536865s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :224256, Decode Length :1, inference time:0.0023214988708496094s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :225280, Decode Length :1, inference time:0.002328569412231445s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :226304, Decode Length :1, inference time:0.0023370643854141236s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :227328, Decode Length :1, inference time:0.0023487204313278197s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :228352, Decode Length :1, inference time:0.002358618140220642s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :229376, Decode Length :1, inference time:0.002365496754646301s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :230400, Decode Length :1, inference time:0.0023784136772155764s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :231424, Decode Length :1, inference time:0.0023870660066604614s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :232448, Decode Length :1, inference time:0.0023952414989471434s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :233472, Decode Length :1, inference time:0.0023989312648773193s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :234496, Decode Length :1, inference time:0.0024098608493804932s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :235520, Decode Length :1, inference time:0.0024188987016677856s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :236544, Decode Length :1, inference time:0.0024343581199645996s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :237568, Decode Length :1, inference time:0.0024390708208084106s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :238592, Decode Length :1, inference time:0.002444774389266968s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :239616, Decode Length :1, inference time:0.0024598878622055054s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :240640, Decode Length :1, inference time:0.002464240193367004s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :241664, Decode Length :1, inference time:0.00248199725151062s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :242688, Decode Length :1, inference time:0.0024868687391281126s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :243712, Decode Length :1, inference time:0.00249374783039093s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :244736, Decode Length :1, inference time:0.002507785439491272s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :245760, Decode Length :1, inference time:0.0025215574502944946s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :246784, Decode Length :1, inference time:0.002528430104255676s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :247808, Decode Length :1, inference time:0.0025358209609985354s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :248832, Decode Length :1, inference time:0.0025502111911773684s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :249856, Decode Length :1, inference time:0.002554101228713989s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :250880, Decode Length :1, inference time:0.0025607484579086306s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :251904, Decode Length :1, inference time:0.0025689514875411987s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :252928, Decode Length :1, inference time:0.0025736483335494996s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :253952, Decode Length :1, inference time:0.0025871872901916506s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :254976, Decode Length :1, inference time:0.0025957727432250974s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :256000, Decode Length :1, inference time:0.0026001924276351927s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :257024, Decode Length :1, inference time:0.002622915863990784s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :258048, Decode Length :1, inference time:0.002629505157470703s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :259072, Decode Length :1, inference time:0.002645414710044861s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :260096, Decode Length :1, inference time:0.002650443077087402s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :261120, Decode Length :1, inference time:0.0026632355451583862s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :262144, Decode Length :1, inference time:0.0026704392433166504s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :263168, Decode Length :1, inference time:0.0026775522232055666s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :264192, Decode Length :1, inference time:0.0026880980730056765s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :265216, Decode Length :1, inference time:0.0026956210136413576s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :266240, Decode Length :1, inference time:0.002706822156906128s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :267264, Decode Length :1, inference time:0.0027061164379119872s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :268288, Decode Length :1, inference time:0.0027205584049224854s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :269312, Decode Length :1, inference time:0.0027267192602157594s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :270336, Decode Length :1, inference time:0.0027369052171707154s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :271360, Decode Length :1, inference time:0.0027435383796691895s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :272384, Decode Length :1, inference time:0.002751344084739685s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :273408, Decode Length :1, inference time:0.002770477890968323s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :274432, Decode Length :1, inference time:0.0027820457220077517s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :275456, Decode Length :1, inference time:0.0027894327640533446s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :276480, Decode Length :1, inference time:0.002801088333129883s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :277504, Decode Length :1, inference time:0.0028074817657470705s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :278528, Decode Length :1, inference time:0.0028154771327972413s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :279552, Decode Length :1, inference time:0.002832617521286011s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :280576, Decode Length :1, inference time:0.002837293744087219s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :281600, Decode Length :1, inference time:0.002849201798439026s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :282624, Decode Length :1, inference time:0.0028477792739868162s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :283648, Decode Length :1, inference time:0.0028561152219772337s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :284672, Decode Length :1, inference time:0.0028685479164123535s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :285696, Decode Length :1, inference time:0.0028768495321273803s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :286720, Decode Length :1, inference time:0.0028846737146377564s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :287744, Decode Length :1, inference time:0.0028951711654663084s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :288768, Decode Length :1, inference time:0.00290224552154541s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :289792, Decode Length :1, inference time:0.0029084906578063963s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :290816, Decode Length :1, inference time:0.002930122971534729s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :291840, Decode Length :1, inference time:0.0029429152011871337s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :292864, Decode Length :1, inference time:0.0029502002000808716s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :293888, Decode Length :1, inference time:0.002953840732574463s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :294912, Decode Length :1, inference time:0.002969144105911255s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :295936, Decode Length :1, inference time:0.0029738024473190306s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :296960, Decode Length :1, inference time:0.0029774192571640013s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :297984, Decode Length :1, inference time:0.0029848175048828125s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :299008, Decode Length :1, inference time:0.0029996285438537596s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :300032, Decode Length :1, inference time:0.0030051681995391846s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :301056, Decode Length :1, inference time:0.0030116167068481446s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :302080, Decode Length :1, inference time:0.003037998676300049s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :303104, Decode Length :1, inference time:0.003043294668197632s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :304128, Decode Length :1, inference time:0.0030495303869247435s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :305152, Decode Length :1, inference time:0.003053574323654175s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :306176, Decode Length :1, inference time:0.003062191843986511s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :307200, Decode Length :1, inference time:0.0030758622884750368s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :308224, Decode Length :1, inference time:0.0030834130048751833s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :309248, Decode Length :1, inference time:0.0030985796451568604s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :310272, Decode Length :1, inference time:0.0031101158857345583s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :311296, Decode Length :1, inference time:0.003115739941596985s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :312320, Decode Length :1, inference time:0.003116702675819397s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :313344, Decode Length :1, inference time:0.003131662726402283s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :314368, Decode Length :1, inference time:0.003137870907783508s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :315392, Decode Length :1, inference time:0.0031472851037979126s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :316416, Decode Length :1, inference time:0.003153868317604065s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :317440, Decode Length :1, inference time:0.0031753363609313963s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :318464, Decode Length :1, inference time:0.003187049984931946s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :319488, Decode Length :1, inference time:0.0031912211179733274s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :320512, Decode Length :1, inference time:0.0032035963535308838s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :321536, Decode Length :1, inference time:0.0032120437622070314s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :322560, Decode Length :1, inference time:0.0032117072343826295s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :323584, Decode Length :1, inference time:0.0032173348665237426s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :324608, Decode Length :1, inference time:0.003233310580253601s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :325632, Decode Length :1, inference time:0.0032348625659942625s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :326656, Decode Length :1, inference time:0.003254410982131958s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :327680, Decode Length :1, inference time:0.003256382942199707s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :328704, Decode Length :1, inference time:0.0032632237672805785s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :329728, Decode Length :1, inference time:0.003277838349342346s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :330752, Decode Length :1, inference time:0.003284132957458496s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :331776, Decode Length :1, inference time:0.003289794564247131s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :332800, Decode Length :1, inference time:0.0033206143379211424s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :333824, Decode Length :1, inference time:0.003324139595031738s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :334848, Decode Length :1, inference time:0.0033295809030532836s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :335872, Decode Length :1, inference time:0.003343750596046448s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :336896, Decode Length :1, inference time:0.0033508477210998536s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :337920, Decode Length :1, inference time:0.003363934874534607s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :338944, Decode Length :1, inference time:0.0033727127313613894s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :339968, Decode Length :1, inference time:0.003379953980445862s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :340992, Decode Length :1, inference time:0.0033806228637695314s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :342016, Decode Length :1, inference time:0.0033901686668395994s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :343040, Decode Length :1, inference time:0.0033982930183410645s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :344064, Decode Length :1, inference time:0.003409300684928894s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :345088, Decode Length :1, inference time:0.0034155588150024414s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :346112, Decode Length :1, inference time:0.0034219949245452882s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :347136, Decode Length :1, inference time:0.003433076858520508s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :348160, Decode Length :1, inference time:0.0034594660997390747s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :349184, Decode Length :1, inference time:0.0034664056301116943s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :350208, Decode Length :1, inference time:0.003463820219039917s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :351232, Decode Length :1, inference time:0.003468798041343689s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :352256, Decode Length :1, inference time:0.0034867570400238038s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :353280, Decode Length :1, inference time:0.0035045106410980225s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :354304, Decode Length :1, inference time:0.003512938976287842s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :355328, Decode Length :1, inference time:0.003519602656364441s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :356352, Decode Length :1, inference time:0.003517717480659485s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :357376, Decode Length :1, inference time:0.003525868892669678s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :358400, Decode Length :1, inference time:0.003544738411903381s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :359424, Decode Length :1, inference time:0.0035506532192230225s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :360448, Decode Length :1, inference time:0.0035627148151397706s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :361472, Decode Length :1, inference time:0.0035700316429138185s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :362496, Decode Length :1, inference time:0.003579361915588379s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :363520, Decode Length :1, inference time:0.0035981230735778807s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :364544, Decode Length :1, inference time:0.0036055736541748045s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :365568, Decode Length :1, inference time:0.0036083893775939943s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :366592, Decode Length :1, inference time:0.003626240372657776s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :367616, Decode Length :1, inference time:0.0036310805082321166s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :368640, Decode Length :1, inference time:0.0036418485641479493s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :369664, Decode Length :1, inference time:0.003647415995597839s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :370688, Decode Length :1, inference time:0.0036584151983261107s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :371712, Decode Length :1, inference time:0.003668737769126892s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :372736, Decode Length :1, inference time:0.003671054720878601s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :373760, Decode Length :1, inference time:0.0036768941879272462s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :374784, Decode Length :1, inference time:0.0036920024156570433s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :375808, Decode Length :1, inference time:0.0036974871158599854s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :376832, Decode Length :1, inference time:0.0037062201499938963s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :377856, Decode Length :1, inference time:0.003715480327606201s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :378880, Decode Length :1, inference time:0.003722148060798645s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :379904, Decode Length :1, inference time:0.0037332241535186768s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :380928, Decode Length :1, inference time:0.0037464749813079833s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :381952, Decode Length :1, inference time:0.0037637736797332764s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :382976, Decode Length :1, inference time:0.003772042512893677s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :384000, Decode Length :1, inference time:0.003784571051597595s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :385024, Decode Length :1, inference time:0.0037912232875823974s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :386048, Decode Length :1, inference time:0.00379694926738739s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :387072, Decode Length :1, inference time:0.003804403066635132s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :388096, Decode Length :1, inference time:0.003811652898788452s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :389120, Decode Length :1, inference time:0.003824017643928528s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :390144, Decode Length :1, inference time:0.003830787301063538s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :391168, Decode Length :1, inference time:0.003834468364715576s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :392192, Decode Length :1, inference time:0.003846318244934082s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :393216, Decode Length :1, inference time:0.0038670992851257326s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :394240, Decode Length :1, inference time:0.0038711557388305664s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :395264, Decode Length :1, inference time:0.0038759905099868775s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :396288, Decode Length :1, inference time:0.003880449414253235s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :397312, Decode Length :1, inference time:0.0038965865373611452s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :398336, Decode Length :1, inference time:0.0039024707078933714s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :399360, Decode Length :1, inference time:0.003910562038421631s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :400384, Decode Length :1, inference time:0.003936665773391724s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :401408, Decode Length :1, inference time:0.003930398464202881s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :402432, Decode Length :1, inference time:0.0039384902715682985s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :403456, Decode Length :1, inference time:0.003954531192779541s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :404480, Decode Length :1, inference time:0.003964491009712219s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :405504, Decode Length :1, inference time:0.003973201513290405s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :406528, Decode Length :1, inference time:0.003980822205543518s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :407552, Decode Length :1, inference time:0.003985933303833008s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :408576, Decode Length :1, inference time:0.004008430004119873s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :409600, Decode Length :1, inference time:0.004019993185997009s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :410624, Decode Length :1, inference time:0.004024935722351074s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :411648, Decode Length :1, inference time:0.004032347559928894s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :412672, Decode Length :1, inference time:0.0040346083641052246s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :413696, Decode Length :1, inference time:0.004041801452636719s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :414720, Decode Length :1, inference time:0.004055191397666931s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :415744, Decode Length :1, inference time:0.004060786247253418s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :416768, Decode Length :1, inference time:0.004077413082122803s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :417792, Decode Length :1, inference time:0.004080288052558899s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :418816, Decode Length :1, inference time:0.004087201833724976s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :419840, Decode Length :1, inference time:0.004107610940933227s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :420864, Decode Length :1, inference time:0.004112542033195495s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :421888, Decode Length :1, inference time:0.0041198041439056395s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :422912, Decode Length :1, inference time:0.004137227416038513s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :423936, Decode Length :1, inference time:0.004146978378295898s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :424960, Decode Length :1, inference time:0.004157891392707825s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :425984, Decode Length :1, inference time:0.004166963458061218s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :427008, Decode Length :1, inference time:0.004171856164932251s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :428032, Decode Length :1, inference time:0.004185543179512024s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :429056, Decode Length :1, inference time:0.004192825675010681s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :430080, Decode Length :1, inference time:0.00419053041934967s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :431104, Decode Length :1, inference time:0.004203144192695618s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :432128, Decode Length :1, inference time:0.004212412476539612s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :433152, Decode Length :1, inference time:0.004217085003852844s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :434176, Decode Length :1, inference time:0.004232291579246521s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :435200, Decode Length :1, inference time:0.004241044878959656s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :436224, Decode Length :1, inference time:0.0042447069883346554s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :437248, Decode Length :1, inference time:0.004265223145484925s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :438272, Decode Length :1, inference time:0.004280594825744629s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :439296, Decode Length :1, inference time:0.004288040161132813s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :440320, Decode Length :1, inference time:0.004305296063423157s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :441344, Decode Length :1, inference time:0.004309764266014099s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :442368, Decode Length :1, inference time:0.004317390203475952s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :443392, Decode Length :1, inference time:0.004329191565513611s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :444416, Decode Length :1, inference time:0.004341920018196106s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :445440, Decode Length :1, inference time:0.004333432078361512s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :446464, Decode Length :1, inference time:0.004340909242630005s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :447488, Decode Length :1, inference time:0.00434646725654602s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :448512, Decode Length :1, inference time:0.004363626360893249s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :449536, Decode Length :1, inference time:0.004372366189956665s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :450560, Decode Length :1, inference time:0.0043814113140106204s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :451584, Decode Length :1, inference time:0.004388493061065674s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :452608, Decode Length :1, inference time:0.004397725939750671s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :453632, Decode Length :1, inference time:0.004414336562156678s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :454656, Decode Length :1, inference time:0.004431116700172424s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :455680, Decode Length :1, inference time:0.004444614648818969s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :456704, Decode Length :1, inference time:0.004451081752777099s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :457728, Decode Length :1, inference time:0.0044443378448486325s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :458752, Decode Length :1, inference time:0.004452752947807312s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :459776, Decode Length :1, inference time:0.004467856764793396s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :460800, Decode Length :1, inference time:0.004474735856056213s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :461824, Decode Length :1, inference time:0.004484275937080383s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :462848, Decode Length :1, inference time:0.004491985559463501s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :463872, Decode Length :1, inference time:0.004498648047447204s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :464896, Decode Length :1, inference time:0.004513146877288818s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :465920, Decode Length :1, inference time:0.004517962813377381s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :466944, Decode Length :1, inference time:0.004532010078430176s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :467968, Decode Length :1, inference time:0.0045449576377868655s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :468992, Decode Length :1, inference time:0.004549220085144043s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :470016, Decode Length :1, inference time:0.004561724543571472s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :471040, Decode Length :1, inference time:0.004567310333251953s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :472064, Decode Length :1, inference time:0.004572287797927856s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :473088, Decode Length :1, inference time:0.004601538419723511s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :474112, Decode Length :1, inference time:0.004610466122627258s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :475136, Decode Length :1, inference time:0.004602911353111267s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :476160, Decode Length :1, inference time:0.004621096730232239s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :477184, Decode Length :1, inference time:0.00462548828125s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :478208, Decode Length :1, inference time:0.004637754559516907s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :479232, Decode Length :1, inference time:0.004646906971931457s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :480256, Decode Length :1, inference time:0.004651418924331665s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :481280, Decode Length :1, inference time:0.004657500267028809s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :482304, Decode Length :1, inference time:0.0046712676286697384s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :483328, Decode Length :1, inference time:0.004693719267845154s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :484352, Decode Length :1, inference time:0.004692363977432251s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :485376, Decode Length :1, inference time:0.004698981523513794s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :486400, Decode Length :1, inference time:0.004706802010536194s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :487424, Decode Length :1, inference time:0.004721227884292602s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :488448, Decode Length :1, inference time:0.00472669517993927s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :489472, Decode Length :1, inference time:0.004734557390213012s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :490496, Decode Length :1, inference time:0.004752519369125366s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :491520, Decode Length :1, inference time:0.004755377531051636s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :492544, Decode Length :1, inference time:0.0047616008520126344s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :493568, Decode Length :1, inference time:0.004775871157646179s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :494592, Decode Length :1, inference time:0.0047870856523513795s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :495616, Decode Length :1, inference time:0.00479437804222107s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :496640, Decode Length :1, inference time:0.004798812866210938s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :497664, Decode Length :1, inference time:0.0048069459199905394s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :498688, Decode Length :1, inference time:0.004835002183914185s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :499712, Decode Length :1, inference time:0.004847128629684448s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :500736, Decode Length :1, inference time:0.0048537936210632324s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :501760, Decode Length :1, inference time:0.004853881239891052s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :502784, Decode Length :1, inference time:0.004862772107124329s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :503808, Decode Length :1, inference time:0.004863927125930786s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :504832, Decode Length :1, inference time:0.004877299547195435s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :505856, Decode Length :1, inference time:0.00488699209690094s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :506880, Decode Length :1, inference time:0.004898557662963867s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :507904, Decode Length :1, inference time:0.004906327486038208s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :508928, Decode Length :1, inference time:0.004911736130714417s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :509952, Decode Length :1, inference time:0.0049253469705581665s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :510976, Decode Length :1, inference time:0.004934895873069763s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :512000, Decode Length :1, inference time:0.004937366485595703s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :513024, Decode Length :1, inference time:0.00495353639125824s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :514048, Decode Length :1, inference time:0.004960652232170105s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :515072, Decode Length :1, inference time:0.004968041658401489s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :516096, Decode Length :1, inference time:0.0049998054504394535s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :517120, Decode Length :1, inference time:0.005007052302360535s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :518144, Decode Length :1, inference time:0.005012026429176331s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :519168, Decode Length :1, inference time:0.00501975929737091s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :520192, Decode Length :1, inference time:0.005013401865959168s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :521216, Decode Length :1, inference time:0.005022991180419922s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :522240, Decode Length :1, inference time:0.005034462213516235s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :523264, Decode Length :1, inference time:0.005042809963226319s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :524288, Decode Length :1, inference time:0.005059211850166321s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :525312, Decode Length :1, inference time:0.0050657567977905275s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :526336, Decode Length :1, inference time:0.005067242622375488s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :527360, Decode Length :1, inference time:0.005086077332496643s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :528384, Decode Length :1, inference time:0.005090345025062561s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :529408, Decode Length :1, inference time:0.005114304542541504s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :530432, Decode Length :1, inference time:0.005126588225364685s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :531456, Decode Length :1, inference time:0.005131049036979675s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :532480, Decode Length :1, inference time:0.0051468688249588015s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :533504, Decode Length :1, inference time:0.005152409791946411s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :534528, Decode Length :1, inference time:0.005146766185760498s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :535552, Decode Length :1, inference time:0.005160991311073304s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :536576, Decode Length :1, inference time:0.005168457865715027s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :537600, Decode Length :1, inference time:0.005174376964569092s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :538624, Decode Length :1, inference time:0.005187095165252686s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :539648, Decode Length :1, inference time:0.005197345137596131s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :540672, Decode Length :1, inference time:0.005210649371147156s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :541696, Decode Length :1, inference time:0.0052149738073349s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :542720, Decode Length :1, inference time:0.005222396612167358s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :543744, Decode Length :1, inference time:0.005235841870307923s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :544768, Decode Length :1, inference time:0.005239619612693786s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :545792, Decode Length :1, inference time:0.005272612571716308s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :546816, Decode Length :1, inference time:0.005279176235198975s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :547840, Decode Length :1, inference time:0.005283610463142395s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :548864, Decode Length :1, inference time:0.00528960919380188s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :549888, Decode Length :1, inference time:0.00529030704498291s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :550912, Decode Length :1, inference time:0.005298120260238647s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :551936, Decode Length :1, inference time:0.005309869170188904s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :552960, Decode Length :1, inference time:0.00531829571723938s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :553984, Decode Length :1, inference time:0.00532565176486969s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :555008, Decode Length :1, inference time:0.005332859396934509s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :556032, Decode Length :1, inference time:0.005342109084129333s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :557056, Decode Length :1, inference time:0.005346787571907044s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :558080, Decode Length :1, inference time:0.005364690065383911s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :559104, Decode Length :1, inference time:0.0053682299852371215s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :560128, Decode Length :1, inference time:0.005378093123435974s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :561152, Decode Length :1, inference time:0.005387166500091553s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :562176, Decode Length :1, inference time:0.005396084427833557s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :563200, Decode Length :1, inference time:0.005408053874969483s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :564224, Decode Length :1, inference time:0.005419999122619629s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :565248, Decode Length :1, inference time:0.0054303773641586305s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :566272, Decode Length :1, inference time:0.005446729183197022s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :567296, Decode Length :1, inference time:0.005446339249610901s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :568320, Decode Length :1, inference time:0.005460778713226319s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :569344, Decode Length :1, inference time:0.005470917463302612s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :570368, Decode Length :1, inference time:0.005475989937782287s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :571392, Decode Length :1, inference time:0.005483689188957214s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :572416, Decode Length :1, inference time:0.005497132062911987s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :573440, Decode Length :1, inference time:0.005503058671951294s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :574464, Decode Length :1, inference time:0.005516684293746949s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :575488, Decode Length :1, inference time:0.005525703430175781s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :576512, Decode Length :1, inference time:0.00553242027759552s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :577536, Decode Length :1, inference time:0.005543409943580628s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :578560, Decode Length :1, inference time:0.005547286987304688s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :579584, Decode Length :1, inference time:0.005557748198509216s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :580608, Decode Length :1, inference time:0.00557153069972992s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :581632, Decode Length :1, inference time:0.0055775693655014036s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :582656, Decode Length :1, inference time:0.005582241773605346s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :583680, Decode Length :1, inference time:0.005594854116439819s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :584704, Decode Length :1, inference time:0.005602110862731933s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :585728, Decode Length :1, inference time:0.0056176954507827755s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :586752, Decode Length :1, inference time:0.005623397827148438s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :587776, Decode Length :1, inference time:0.005633771896362305s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :588800, Decode Length :1, inference time:0.005647106647491455s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :589824, Decode Length :1, inference time:0.005669871211051941s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :590848, Decode Length :1, inference time:0.005677606105804443s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :591872, Decode Length :1, inference time:0.005673843860626221s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :592896, Decode Length :1, inference time:0.005680180191993713s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :593920, Decode Length :1, inference time:0.005687400460243225s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :594944, Decode Length :1, inference time:0.005701889038085937s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :595968, Decode Length :1, inference time:0.00570811128616333s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :596992, Decode Length :1, inference time:0.0057185027599334715s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :598016, Decode Length :1, inference time:0.005732806205749512s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :599040, Decode Length :1, inference time:0.0057397900819778445s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :600064, Decode Length :1, inference time:0.0057525656223297115s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :601088, Decode Length :1, inference time:0.005758548021316528s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :602112, Decode Length :1, inference time:0.005767741680145264s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :603136, Decode Length :1, inference time:0.005779918074607849s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :604160, Decode Length :1, inference time:0.005804063320159912s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :605184, Decode Length :1, inference time:0.005812229037284851s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :606208, Decode Length :1, inference time:0.0058167816400527955s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :607232, Decode Length :1, inference time:0.005833189249038697s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :608256, Decode Length :1, inference time:0.005838513135910034s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :609280, Decode Length :1, inference time:0.0058308010101318355s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :610304, Decode Length :1, inference time:0.005838666677474976s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :611328, Decode Length :1, inference time:0.005853506684303284s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :612352, Decode Length :1, inference time:0.005856171011924744s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :613376, Decode Length :1, inference time:0.005862858772277832s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :614400, Decode Length :1, inference time:0.0058813788890838626s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :615424, Decode Length :1, inference time:0.0058856925964355464s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :616448, Decode Length :1, inference time:0.0058936262130737304s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :617472, Decode Length :1, inference time:0.005909808039665222s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :618496, Decode Length :1, inference time:0.0059162331819534305s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :619520, Decode Length :1, inference time:0.005930935740470886s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :620544, Decode Length :1, inference time:0.005940874934196472s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :621568, Decode Length :1, inference time:0.005942953824996948s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :622592, Decode Length :1, inference time:0.005958759903907776s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :623616, Decode Length :1, inference time:0.005966354250907898s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :624640, Decode Length :1, inference time:0.005971423983573913s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :625664, Decode Length :1, inference time:0.00598020875453949s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :626688, Decode Length :1, inference time:0.005987548828125s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :627712, Decode Length :1, inference time:0.005996306419372558s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :628736, Decode Length :1, inference time:0.006012355327606201s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :629760, Decode Length :1, inference time:0.006022190690040588s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :630784, Decode Length :1, inference time:0.006034657001495362s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :631808, Decode Length :1, inference time:0.006041864275932312s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :632832, Decode Length :1, inference time:0.006046547889709473s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :633856, Decode Length :1, inference time:0.006055784463882446s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :634880, Decode Length :1, inference time:0.006067795515060425s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :635904, Decode Length :1, inference time:0.006074310421943664s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :636928, Decode Length :1, inference time:0.006100963234901428s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :637952, Decode Length :1, inference time:0.00610819673538208s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :638976, Decode Length :1, inference time:0.0061030093431472775s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :640000, Decode Length :1, inference time:0.006114596009254456s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :641024, Decode Length :1, inference time:0.00612330174446106s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :642048, Decode Length :1, inference time:0.006134082794189453s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :643072, Decode Length :1, inference time:0.006145055055618286s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :644096, Decode Length :1, inference time:0.00615408718585968s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :645120, Decode Length :1, inference time:0.006163077235221863s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :646144, Decode Length :1, inference time:0.00617316997051239s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :647168, Decode Length :1, inference time:0.006174660444259643s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :648192, Decode Length :1, inference time:0.0061883931159973146s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :649216, Decode Length :1, inference time:0.006198075413703918s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :650240, Decode Length :1, inference time:0.006206764221191406s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :651264, Decode Length :1, inference time:0.006218425273895264s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :652288, Decode Length :1, inference time:0.006226418137550354s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :653312, Decode Length :1, inference time:0.0062416398525238035s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :654336, Decode Length :1, inference time:0.006246211528778076s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :655360, Decode Length :1, inference time:0.006253668904304504s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :656384, Decode Length :1, inference time:0.0062646143436431885s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :657408, Decode Length :1, inference time:0.006273531556129456s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :658432, Decode Length :1, inference time:0.006281093239784241s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :659456, Decode Length :1, inference time:0.00629470419883728s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :660480, Decode Length :1, inference time:0.006301186680793762s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :661504, Decode Length :1, inference time:0.006307023286819458s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :662528, Decode Length :1, inference time:0.006324757933616638s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :663552, Decode Length :1, inference time:0.00632649028301239s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :664576, Decode Length :1, inference time:0.006339049577713013s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :665600, Decode Length :1, inference time:0.006348706007003784s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :666624, Decode Length :1, inference time:0.006353681802749634s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :667648, Decode Length :1, inference time:0.0063713108301162715s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :668672, Decode Length :1, inference time:0.00638243579864502s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :669696, Decode Length :1, inference time:0.006382781386375427s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :670720, Decode Length :1, inference time:0.006394788026809693s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :671744, Decode Length :1, inference time:0.006404840588569641s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :672768, Decode Length :1, inference time:0.006414936661720276s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :673792, Decode Length :1, inference time:0.006417333245277405s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :674816, Decode Length :1, inference time:0.006427081346511841s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :675840, Decode Length :1, inference time:0.00644892406463623s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :676864, Decode Length :1, inference time:0.00645486843585968s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :677888, Decode Length :1, inference time:0.006463367938995362s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :678912, Decode Length :1, inference time:0.006467816829681397s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :679936, Decode Length :1, inference time:0.006497862339019775s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :680960, Decode Length :1, inference time:0.006478822469711304s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :681984, Decode Length :1, inference time:0.0064994716644287106s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :683008, Decode Length :1, inference time:0.0065026251077651975s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :684032, Decode Length :1, inference time:0.006515635967254639s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :685056, Decode Length :1, inference time:0.006528097152709961s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :686080, Decode Length :1, inference time:0.006529975295066833s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :687104, Decode Length :1, inference time:0.006545540452003479s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :688128, Decode Length :1, inference time:0.006547794222831726s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :689152, Decode Length :1, inference time:0.00655361807346344s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :690176, Decode Length :1, inference time:0.006574150085449219s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :691200, Decode Length :1, inference time:0.006580367088317871s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :692224, Decode Length :1, inference time:0.006591024160385131s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :693248, Decode Length :1, inference time:0.006607900500297546s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :694272, Decode Length :1, inference time:0.006607574701309204s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :695296, Decode Length :1, inference time:0.006635315179824829s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :696320, Decode Length :1, inference time:0.006651100277900696s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :697344, Decode Length :1, inference time:0.006660156607627869s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :698368, Decode Length :1, inference time:0.00664958655834198s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :699392, Decode Length :1, inference time:0.0066532094478607175s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :700416, Decode Length :1, inference time:0.006664952874183655s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :701440, Decode Length :1, inference time:0.006677957773208618s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :702464, Decode Length :1, inference time:0.006681723117828369s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :703488, Decode Length :1, inference time:0.006688384413719177s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :704512, Decode Length :1, inference time:0.00670965576171875s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :705536, Decode Length :1, inference time:0.0067131732702255245s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :706560, Decode Length :1, inference time:0.006722118496894836s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :707584, Decode Length :1, inference time:0.006737644076347351s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :708608, Decode Length :1, inference time:0.006739113330841064s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :709632, Decode Length :1, inference time:0.006751113891601562s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :710656, Decode Length :1, inference time:0.006779327869415284s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :711680, Decode Length :1, inference time:0.006790855646133423s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :712704, Decode Length :1, inference time:0.006780480027198791s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :713728, Decode Length :1, inference time:0.00678558087348938s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :714752, Decode Length :1, inference time:0.006792733430862427s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :715776, Decode Length :1, inference time:0.0068159356117248535s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :716800, Decode Length :1, inference time:0.0068171886205673216s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :717824, Decode Length :1, inference time:0.006824375867843628s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :718848, Decode Length :1, inference time:0.0068360501527786255s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :719872, Decode Length :1, inference time:0.0068429609537124635s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :720896, Decode Length :1, inference time:0.00685654091835022s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :721920, Decode Length :1, inference time:0.0068622674942016605s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :722944, Decode Length :1, inference time:0.006871264934539795s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :723968, Decode Length :1, inference time:0.006881812572479248s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :724992, Decode Length :1, inference time:0.006887155652046203s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :726016, Decode Length :1, inference time:0.006895665645599365s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :727040, Decode Length :1, inference time:0.006910823941230774s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :728064, Decode Length :1, inference time:0.00691294801235199s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :729088, Decode Length :1, inference time:0.006921658873558044s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :730112, Decode Length :1, inference time:0.0069379123449325565s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :731136, Decode Length :1, inference time:0.006951934456825256s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :732160, Decode Length :1, inference time:0.006959656596183777s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :733184, Decode Length :1, inference time:0.006964855313301086s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :734208, Decode Length :1, inference time:0.0069726409912109375s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :735232, Decode Length :1, inference time:0.006982003211975098s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :736256, Decode Length :1, inference time:0.006992100954055786s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :737280, Decode Length :1, inference time:0.006997641682624817s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :738304, Decode Length :1, inference time:0.007014593124389648s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :739328, Decode Length :1, inference time:0.007017390966415405s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :740352, Decode Length :1, inference time:0.007025650978088379s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :741376, Decode Length :1, inference time:0.007042553305625915s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :742400, Decode Length :1, inference time:0.007048861861228943s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :743424, Decode Length :1, inference time:0.007060589790344238s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :744448, Decode Length :1, inference time:0.007067355275154114s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :745472, Decode Length :1, inference time:0.007078434586524964s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :746496, Decode Length :1, inference time:0.007093641996383667s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :747520, Decode Length :1, inference time:0.007095180630683899s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :748544, Decode Length :1, inference time:0.007104507684707642s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :749568, Decode Length :1, inference time:0.0071130222082138065s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :750592, Decode Length :1, inference time:0.007123651504516601s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :751616, Decode Length :1, inference time:0.0071262259483337404s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :752640, Decode Length :1, inference time:0.007143663525581359s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :753664, Decode Length :1, inference time:0.007148417234420776s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :754688, Decode Length :1, inference time:0.007165195822715759s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :755712, Decode Length :1, inference time:0.0071745836734771725s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :756736, Decode Length :1, inference time:0.0071819907426834105s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :757760, Decode Length :1, inference time:0.007192813158035279s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :758784, Decode Length :1, inference time:0.007199573993682862s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :759808, Decode Length :1, inference time:0.007213225841522217s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :760832, Decode Length :1, inference time:0.0072213938236236576s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :761856, Decode Length :1, inference time:0.007226596713066101s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :762880, Decode Length :1, inference time:0.007234100699424744s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :763904, Decode Length :1, inference time:0.007252330064773559s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :764928, Decode Length :1, inference time:0.00726177716255188s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :765952, Decode Length :1, inference time:0.007267004370689392s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :766976, Decode Length :1, inference time:0.007275160074234009s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :768000, Decode Length :1, inference time:0.0072846032381057736s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :769024, Decode Length :1, inference time:0.0072997031211853025s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :770048, Decode Length :1, inference time:0.00729874861240387s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :771072, Decode Length :1, inference time:0.007307173013687134s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :772096, Decode Length :1, inference time:0.007325800180435181s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :773120, Decode Length :1, inference time:0.007328962802886963s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :774144, Decode Length :1, inference time:0.007331272721290589s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :775168, Decode Length :1, inference time:0.007349444508552551s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :776192, Decode Length :1, inference time:0.007357828259468078s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :777216, Decode Length :1, inference time:0.00737209951877594s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :778240, Decode Length :1, inference time:0.007380748987197876s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :779264, Decode Length :1, inference time:0.0073841607570648195s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :780288, Decode Length :1, inference time:0.007403326749801636s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :781312, Decode Length :1, inference time:0.007404565334320068s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :782336, Decode Length :1, inference time:0.007418989777565003s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :783360, Decode Length :1, inference time:0.007430633544921875s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :784384, Decode Length :1, inference time:0.007441994071006775s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :785408, Decode Length :1, inference time:0.007447053074836731s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :786432, Decode Length :1, inference time:0.007453240871429443s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :787456, Decode Length :1, inference time:0.007460603475570679s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :788480, Decode Length :1, inference time:0.007481713771820068s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :789504, Decode Length :1, inference time:0.00748216462135315s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :790528, Decode Length :1, inference time:0.007485352993011475s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :791552, Decode Length :1, inference time:0.00750234043598175s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :792576, Decode Length :1, inference time:0.007512808561325073s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :793600, Decode Length :1, inference time:0.007516761541366577s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :794624, Decode Length :1, inference time:0.00753196907043457s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :795648, Decode Length :1, inference time:0.007536213874816894s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :796672, Decode Length :1, inference time:0.007544951200485229s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :797696, Decode Length :1, inference time:0.0075571430921554565s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :798720, Decode Length :1, inference time:0.007562371969223022s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :799744, Decode Length :1, inference time:0.007581876277923584s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :800768, Decode Length :1, inference time:0.007605251550674439s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :801792, Decode Length :1, inference time:0.007596139073371888s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :802816, Decode Length :1, inference time:0.007615075349807739s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :803840, Decode Length :1, inference time:0.007618378758430481s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :804864, Decode Length :1, inference time:0.007613301157951355s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :805888, Decode Length :1, inference time:0.007632954120635986s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :806912, Decode Length :1, inference time:0.007636901140213013s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :807936, Decode Length :1, inference time:0.00764392352104187s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :808960, Decode Length :1, inference time:0.007659921169281006s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :809984, Decode Length :1, inference time:0.0076708999872207645s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :811008, Decode Length :1, inference time:0.007683244585990906s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :812032, Decode Length :1, inference time:0.0076878430843353275s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :813056, Decode Length :1, inference time:0.007693958282470703s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :814080, Decode Length :1, inference time:0.007710263729095459s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :815104, Decode Length :1, inference time:0.007712679147720337s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :816128, Decode Length :1, inference time:0.007719502568244934s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :817152, Decode Length :1, inference time:0.0077341760396957395s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :818176, Decode Length :1, inference time:0.007735723733901977s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :819200, Decode Length :1, inference time:0.0077470684051513675s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :820224, Decode Length :1, inference time:0.007766209363937378s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :821248, Decode Length :1, inference time:0.007770248770713807s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :822272, Decode Length :1, inference time:0.007787175297737122s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :823296, Decode Length :1, inference time:0.007786022424697876s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :824320, Decode Length :1, inference time:0.007793824791908264s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :825344, Decode Length :1, inference time:0.007811188101768494s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :826368, Decode Length :1, inference time:0.007816879749298096s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :827392, Decode Length :1, inference time:0.007826577305793763s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :828416, Decode Length :1, inference time:0.007847681879997253s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :829440, Decode Length :1, inference time:0.007843546986579896s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :830464, Decode Length :1, inference time:0.007854790210723876s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :831488, Decode Length :1, inference time:0.007870654344558716s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :832512, Decode Length :1, inference time:0.007878123641014098s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :833536, Decode Length :1, inference time:0.007887123703956604s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :834560, Decode Length :1, inference time:0.007894681811332703s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :835584, Decode Length :1, inference time:0.007900819182395935s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :836608, Decode Length :1, inference time:0.007910484433174133s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :837632, Decode Length :1, inference time:0.007919702291488648s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :838656, Decode Length :1, inference time:0.007930823922157288s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :839680, Decode Length :1, inference time:0.007943766951560974s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :840704, Decode Length :1, inference time:0.007950711727142333s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :841728, Decode Length :1, inference time:0.007956316590309143s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :842752, Decode Length :1, inference time:0.007970073342323303s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :843776, Decode Length :1, inference time:0.007975371479988098s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :844800, Decode Length :1, inference time:0.007990800142288208s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :845824, Decode Length :1, inference time:0.007997764706611633s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :846848, Decode Length :1, inference time:0.008005661845207214s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :847872, Decode Length :1, inference time:0.008014792680740356s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :848896, Decode Length :1, inference time:0.008019378066062927s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :849920, Decode Length :1, inference time:0.008034282207489014s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :850944, Decode Length :1, inference time:0.008049614071846008s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :851968, Decode Length :1, inference time:0.008059976577758789s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :852992, Decode Length :1, inference time:0.008064599752426148s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :854016, Decode Length :1, inference time:0.008077140331268311s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :855040, Decode Length :1, inference time:0.008086545348167419s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :856064, Decode Length :1, inference time:0.008093971133232116s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :857088, Decode Length :1, inference time:0.008103146433830262s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :858112, Decode Length :1, inference time:0.008109787344932557s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :859136, Decode Length :1, inference time:0.008128509759902954s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :860160, Decode Length :1, inference time:0.008130311965942383s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :861184, Decode Length :1, inference time:0.008136056542396545s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :862208, Decode Length :1, inference time:0.008154606103897094s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :863232, Decode Length :1, inference time:0.008155131816864013s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :864256, Decode Length :1, inference time:0.008162435412406922s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :865280, Decode Length :1, inference time:0.008184305906295776s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :866304, Decode Length :1, inference time:0.008175724864006043s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :867328, Decode Length :1, inference time:0.008194897174835205s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :868352, Decode Length :1, inference time:0.008205459713935851s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :869376, Decode Length :1, inference time:0.00821172297000885s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :870400, Decode Length :1, inference time:0.00822546672821045s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :871424, Decode Length :1, inference time:0.008229034423828125s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :872448, Decode Length :1, inference time:0.008245305061340332s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :873472, Decode Length :1, inference time:0.008252703666687011s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :874496, Decode Length :1, inference time:0.008262930989265442s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :875520, Decode Length :1, inference time:0.008264909863471986s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :876544, Decode Length :1, inference time:0.00827582573890686s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :877568, Decode Length :1, inference time:0.008292619466781616s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :878592, Decode Length :1, inference time:0.008305435299873352s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :879616, Decode Length :1, inference time:0.008301742553710937s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :880640, Decode Length :1, inference time:0.00831382441520691s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :881664, Decode Length :1, inference time:0.008326809048652649s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :882688, Decode Length :1, inference time:0.008332825422286987s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :883712, Decode Length :1, inference time:0.008341106176376343s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :884736, Decode Length :1, inference time:0.008349745512008667s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :885760, Decode Length :1, inference time:0.008357150673866273s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :886784, Decode Length :1, inference time:0.008365312576293945s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :887808, Decode Length :1, inference time:0.008375793695449829s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :888832, Decode Length :1, inference time:0.008388725161552429s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :889856, Decode Length :1, inference time:0.00839645802974701s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :890880, Decode Length :1, inference time:0.00840784466266632s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :891904, Decode Length :1, inference time:0.008409687161445618s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :892928, Decode Length :1, inference time:0.008443658351898193s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :893952, Decode Length :1, inference time:0.0084482262134552s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :894976, Decode Length :1, inference time:0.008446462512016296s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :896000, Decode Length :1, inference time:0.008455646157264709s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :897024, Decode Length :1, inference time:0.008461175918579101s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :898048, Decode Length :1, inference time:0.008468589067459106s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :899072, Decode Length :1, inference time:0.008491163372993469s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :900096, Decode Length :1, inference time:0.008499431490898133s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :901120, Decode Length :1, inference time:0.00849840772151947s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :902144, Decode Length :1, inference time:0.008505985617637634s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :903168, Decode Length :1, inference time:0.008516313314437866s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :904192, Decode Length :1, inference time:0.008528109788894653s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :905216, Decode Length :1, inference time:0.008536678433418274s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :906240, Decode Length :1, inference time:0.008544028997421264s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :907264, Decode Length :1, inference time:0.008555299282073974s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :908288, Decode Length :1, inference time:0.008558537364006042s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :909312, Decode Length :1, inference time:0.008569979190826416s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :910336, Decode Length :1, inference time:0.008594931483268738s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :911360, Decode Length :1, inference time:0.008594703674316407s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :912384, Decode Length :1, inference time:0.008609712958335876s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :913408, Decode Length :1, inference time:0.008615689873695374s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :914432, Decode Length :1, inference time:0.008621670126914978s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :915456, Decode Length :1, inference time:0.008637918710708618s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :916480, Decode Length :1, inference time:0.008642173290252686s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :917504, Decode Length :1, inference time:0.008647285103797912s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :918528, Decode Length :1, inference time:0.008663418054580688s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :919552, Decode Length :1, inference time:0.008679650902748108s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :920576, Decode Length :1, inference time:0.008678834438323974s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :921600, Decode Length :1, inference time:0.008684826493263244s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :922624, Decode Length :1, inference time:0.008694656610488892s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :923648, Decode Length :1, inference time:0.008717743277549744s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :924672, Decode Length :1, inference time:0.008736677050590514s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :925696, Decode Length :1, inference time:0.008723079442977906s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :926720, Decode Length :1, inference time:0.008732667088508606s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :927744, Decode Length :1, inference time:0.008746707201004029s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :928768, Decode Length :1, inference time:0.008746536016464234s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :929792, Decode Length :1, inference time:0.008772487044334412s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :930816, Decode Length :1, inference time:0.008805060029029846s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :931840, Decode Length :1, inference time:0.008778548121452331s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :932864, Decode Length :1, inference time:0.008789997696876526s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :933888, Decode Length :1, inference time:0.008794395446777344s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :934912, Decode Length :1, inference time:0.008813108801841735s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :935936, Decode Length :1, inference time:0.008819625735282898s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :936960, Decode Length :1, inference time:0.008825243234634399s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :937984, Decode Length :1, inference time:0.008840201497077942s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :939008, Decode Length :1, inference time:0.00884571886062622s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :940032, Decode Length :1, inference time:0.008857754588127136s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :941056, Decode Length :1, inference time:0.008868516325950622s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :942080, Decode Length :1, inference time:0.008883800029754639s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :943104, Decode Length :1, inference time:0.008887680053710937s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :944128, Decode Length :1, inference time:0.008902557492256165s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :945152, Decode Length :1, inference time:0.008903943777084351s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :946176, Decode Length :1, inference time:0.00892181158065796s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :947200, Decode Length :1, inference time:0.00892951238155365s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :948224, Decode Length :1, inference time:0.008927470326423645s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :949248, Decode Length :1, inference time:0.00894559097290039s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :950272, Decode Length :1, inference time:0.008951200127601623s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :951296, Decode Length :1, inference time:0.008956653714179993s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :952320, Decode Length :1, inference time:0.00897581171989441s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :953344, Decode Length :1, inference time:0.008977387309074401s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :954368, Decode Length :1, inference time:0.008986560344696045s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :955392, Decode Length :1, inference time:0.00900862228870392s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :956416, Decode Length :1, inference time:0.009009016275405884s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :957440, Decode Length :1, inference time:0.009018680095672608s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :958464, Decode Length :1, inference time:0.00902895724773407s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :959488, Decode Length :1, inference time:0.009037039875984192s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :960512, Decode Length :1, inference time:0.009058297872543334s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :961536, Decode Length :1, inference time:0.009083711981773376s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :962560, Decode Length :1, inference time:0.009079525589942933s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :963584, Decode Length :1, inference time:0.009076135277748108s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :964608, Decode Length :1, inference time:0.009089797258377075s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :965632, Decode Length :1, inference time:0.009093608975410462s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :966656, Decode Length :1, inference time:0.009101866841316223s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :967680, Decode Length :1, inference time:0.009109620451927185s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :968704, Decode Length :1, inference time:0.009121888279914857s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :969728, Decode Length :1, inference time:0.009128290891647338s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :970752, Decode Length :1, inference time:0.009130395293235779s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :971776, Decode Length :1, inference time:0.009151897192001343s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :972800, Decode Length :1, inference time:0.00915389621257782s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :973824, Decode Length :1, inference time:0.009162791609764099s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :974848, Decode Length :1, inference time:0.009174904823303222s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :975872, Decode Length :1, inference time:0.009184929490089417s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :976896, Decode Length :1, inference time:0.009188517928123474s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :977920, Decode Length :1, inference time:0.009212573528289796s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :978944, Decode Length :1, inference time:0.009211626410484314s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :979968, Decode Length :1, inference time:0.009223185181617737s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :980992, Decode Length :1, inference time:0.009232401490211487s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :982016, Decode Length :1, inference time:0.009235548973083495s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :983040, Decode Length :1, inference time:0.009253089904785156s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :984064, Decode Length :1, inference time:0.009259971141815185s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :985088, Decode Length :1, inference time:0.009267209887504577s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :986112, Decode Length :1, inference time:0.009284262776374817s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :987136, Decode Length :1, inference time:0.009284804701805115s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :988160, Decode Length :1, inference time:0.009292335867881776s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :989184, Decode Length :1, inference time:0.009304656744003296s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :990208, Decode Length :1, inference time:0.009313792943954468s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :991232, Decode Length :1, inference time:0.009329057574272156s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :992256, Decode Length :1, inference time:0.009339516878128052s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :993280, Decode Length :1, inference time:0.009351770401000976s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :994304, Decode Length :1, inference time:0.009364523530006408s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :995328, Decode Length :1, inference time:0.009368600487709046s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :996352, Decode Length :1, inference time:0.009375691890716553s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :997376, Decode Length :1, inference time:0.009385945677757263s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :998400, Decode Length :1, inference time:0.009389379739761353s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :999424, Decode Length :1, inference time:0.009398972511291504s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1000448, Decode Length :1, inference time:0.0094148850440979s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1001472, Decode Length :1, inference time:0.009417122483253479s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1002496, Decode Length :1, inference time:0.009432551860809326s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1003520, Decode Length :1, inference time:0.009438973665237427s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1004544, Decode Length :1, inference time:0.00944944727420807s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1005568, Decode Length :1, inference time:0.009460434556007384s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1006592, Decode Length :1, inference time:0.009472595572471619s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1007616, Decode Length :1, inference time:0.009479911923408508s
>>>> Flash Attention installed
>>>> Flash RoPE installed
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=1)
    (layers): ModuleList(
      (0-1): 2 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)
          (up_proj): Linear(in_features=768, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=768, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1008640, Decode Length :1, inference time:0.00949441123008728s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1009664, Decode Length :1, inference time:0.009495694041252136s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1010688, Decode Length :1, inference time:0.00950664520263672s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Warming up...
Start benchmark...
Prefix Length :1011712, Decode Length :1, inference time:0.009518227815628052s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10128 [00:00<?, ?it/s]  0%|          | 0/10128 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10138 [00:00<?, ?it/s]  0%|          | 0/10138 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10148 [00:00<?, ?it/s]  0%|          | 0/10148 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10159 [00:00<?, ?it/s]  0%|          | 0/10159 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10169 [00:00<?, ?it/s]  0%|          | 0/10169 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10179 [00:00<?, ?it/s]  0%|          | 0/10179 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10189 [00:00<?, ?it/s]  0%|          | 0/10189 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10200 [00:00<?, ?it/s]  0%|          | 0/10200 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10210 [00:00<?, ?it/s]  0%|          | 0/10210 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10220 [00:00<?, ?it/s]  0%|          | 0/10220 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10230 [00:00<?, ?it/s]  0%|          | 0/10230 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10240 [00:00<?, ?it/s]  0%|          | 0/10240 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10251 [00:00<?, ?it/s]  0%|          | 0/10251 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10261 [00:00<?, ?it/s]  0%|          | 0/10261 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10271 [00:00<?, ?it/s]  0%|          | 0/10271 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10281 [00:00<?, ?it/s]  0%|          | 0/10281 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10292 [00:00<?, ?it/s]  0%|          | 0/10292 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10302 [00:00<?, ?it/s]  0%|          | 0/10302 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10312 [00:00<?, ?it/s]  0%|          | 0/10312 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10322 [00:00<?, ?it/s]  0%|          | 0/10322 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10333 [00:00<?, ?it/s]  0%|          | 0/10333 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10343 [00:00<?, ?it/s]  0%|          | 0/10343 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10353 [00:00<?, ?it/s]  0%|          | 0/10353 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10363 [00:00<?, ?it/s]  0%|          | 0/10363 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10374 [00:00<?, ?it/s]  0%|          | 0/10374 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10384 [00:00<?, ?it/s]  0%|          | 0/10384 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10394 [00:00<?, ?it/s]  0%|          | 0/10394 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10404 [00:00<?, ?it/s]  0%|          | 0/10404 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10415 [00:00<?, ?it/s]  0%|          | 0/10415 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10425 [00:00<?, ?it/s]  0%|          | 0/10425 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10435 [00:00<?, ?it/s]  0%|          | 0/10435 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
  0%|          | 0/10445 [00:00<?, ?it/s]  0%|          | 0/10445 [00:00<?, ?it/s]
position_ids is None
Traceback (most recent call last):
  File "/home/hanshis/workspace/LongContextInfer/benchmark/llama_graph.py", line 78, in <module>
    # graph_engine.inference(input_ids=prefix)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 105, in inference
    return self.engine.model_run(input_ids=input_ids, storage_ids=storage_ids)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/utils/graph_infer.py", line 26, in model_run
    logits = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 349, in forward
    outputs = self.model(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 312, in forward
    layer_outputs = decoder_layer(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 232, in forward
    hidden_states = self.self_attn(
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hanshis/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hanshis/workspace/LongContextInfer/models/modeling_llama.py", line 170, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=graph_cache.key_cache[0].shape[1])
AttributeError: 'NoneType' object has no attribute 'key_cache'
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Prefix Length :1045504, Decode Length :1, inference time:0.00982291054725647s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Prefix Length :1046528, Decode Length :1, inference time:0.00983272683620453s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Prefix Length :1047552, Decode Length :1, inference time:0.009861016273498535s
>>>> Flash Attention installed
>>>> Flash RoPE installed
capturing graph...
Prefix Length :1048576, Decode Length :1, inference time:0.009851354479789734s
